{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def gamma(a, b, features):\n",
    "    LABEL_IDX = 6\n",
    "    return np.transpose(a).dot(features) + b\n",
    "\n",
    "def hinge_loss(a,b, example):\n",
    "    LABEL_IDX = 6\n",
    "    return max(0, 1 - example[LABEL_IDX] * gamma(a, b, example))\n",
    "\n",
    "def penalty_term(a):\n",
    "    return 1/2 * np.asscalar(np.transpose(a).dot(a))\n",
    "\n",
    "#   Calculate Gradient using formulas in CS 498 AML Textbook page 38\n",
    "def gradient_r(u, lam, example):\n",
    "    LABEL_IDX = 6\n",
    "    features = np.array(example[0:LABEL_IDX])\n",
    "    label = example[LABEL_IDX]\n",
    "    a = np.array(u.tolist()[:-1])\n",
    "    b = np.array(u.tolist()[-1])\n",
    "    diff =  label * gamma(a, b, features)\n",
    "\n",
    "    a_deltas = np.array([])\n",
    "    b_delta = 0\n",
    "    #   Calculate Gradient\n",
    "    if diff >= 1:\n",
    "        a_deltas = lam * a\n",
    "        b_delta = 0\n",
    "\n",
    "    else:\n",
    "        a_deltas = np.subtract(lam * a, (label * features).reshape(6, 1))\n",
    "        b_delta = -label\n",
    "\n",
    "    gradient = (np.append(np.array(a_deltas), np.array([b_delta]))).reshape(7, 1)\n",
    "    return gradient\n",
    "\n",
    "def cost_function(u, lam, training_data):\n",
    "    LABEL_IDX = 6\n",
    "    a = u[:-1]\n",
    "    b = u[-1]\n",
    "    N = len(training_data)\n",
    "    temp_sum = 0\n",
    "    count = 0\n",
    "    for e in training_data:\n",
    "        count += 1\n",
    "        features = np.transpose(np.array([e[0:LABEL_IDX]]))\n",
    "        label = e[LABEL_IDX]\n",
    "        g = gamma(a, b, features)\n",
    "        error = 1 - label * g\n",
    "        temp_sum += max(0, np.asscalar(error))\n",
    "\n",
    "    return 1/N * temp_sum + lam * penalty_term(a)\n",
    "\n",
    "def classify(u, features):\n",
    "    a = u[:-1]\n",
    "    b = u[-1]\n",
    "    g = gamma(a, b, features)\n",
    "    out = np.sign(g)[0]\n",
    "    return out\n",
    "\n",
    "#   Learning Rate (Step Length Taken From CS 498 AML Textbook Page 39)\n",
    "def l_rate(epoch):\n",
    "    return 1 / (0.01 * epoch + 50)\n",
    "\n",
    "#   Trains SVM Classifier\n",
    "#   returns : (optimized free parameters u, costs)\n",
    "def train_model(u, lam, l_rate, train_data, epochs=50, steps=300, ex_per_epoch=50, log=False, log_filename=\"\", eval_data=[]):\n",
    "\n",
    "    #   Choose 50 Random Training Examples\n",
    "    epoch_examples = choose_random_data(train_data, examples=ex_per_epoch)\n",
    "    costs = []\n",
    "    counter = 1\n",
    "    for i in range(0, epochs):\n",
    "        for j in range(0, steps):\n",
    "            #   Calculate Gradient\n",
    "            gradient = gradient_r(u, lam, epoch_examples[0])\n",
    "            for i in range(0, ex_per_epoch):\n",
    "                gradient += gradient_r(u, lam, epoch_examples[i])\n",
    "            gradient /= ex_per_epoch\n",
    "\n",
    "            #   Update model\n",
    "            step = l_rate(i) * gradient\n",
    "            u = np.subtract(u, step)\n",
    "\n",
    "            #   Log every 30 steps\n",
    "            counter += 1\n",
    "            if (counter % 30 == 0 and log):\n",
    "                logger(counter, log_filename, u, lam, eval_data)\n",
    "\n",
    "        #   Find current cost\n",
    "        cost = cost_function(u, lam, epoch_examples)\n",
    "        costs.append(cost)\n",
    "        epoch_examples = choose_random_data(train_data, examples=ex_per_epoch)\n",
    "    return (u, costs)\n",
    "\n",
    "#   Evaluate Model using separate data set\n",
    "#   return : the models accuracy over the eval data set\n",
    "def evaluate_model(u, lam, eval_data):\n",
    "    num_correct = 0\n",
    "    for example in eval_data:\n",
    "        features = example[0:-1]\n",
    "        label = example[-1]\n",
    "\n",
    "        prediction = classify(u, features)\n",
    "        if prediction == label:\n",
    "            num_correct += 1\n",
    "    return num_correct / len(eval_data)\n",
    "\n",
    "#   return : lambda accuracies\n",
    "def probe_lambdas(lambdas, l_rate, train_data, splits=10):\n",
    "    u = np.random.rand(7,1)\n",
    "    accuracies = [0 for i in range(0, len(lambdas))]\n",
    "    for i in range(0, splits):\n",
    "        print(\"Probing Lambdas - On split\", i)\n",
    "        splitted_train_data = split_data(train_data)\n",
    "        train_data  = splitted_train_data[1]\n",
    "        eval_data  = splitted_train_data[1]\n",
    "        for j in range(0, len(lambdas)):\n",
    "            lam = lambdas[j]\n",
    "\n",
    "            u_copy = np.copy(u)\n",
    "            optimized_u = train_model(u_copy, lam, l_rate, train_data)[0]\n",
    "            accuracy = evaluate_model(optimized_u, lam, eval_data)\n",
    "            accuracies[j] += accuracy\n",
    "\n",
    "    accuracies = [e/splits for e in accuracies]\n",
    "    return accuracies\n",
    "\n",
    "def log_lambdas(lambdas, l_rate, data):\n",
    "    u = np.random.rand(7,1)\n",
    "    splitted_train_data = split_data(data)\n",
    "    train_data  = splitted_train_data[1]\n",
    "    eval_data  = splitted_train_data[1]\n",
    "    for lam in lambdas:\n",
    "        print(\"Logging\", lam, \"right now...\")\n",
    "        filename = \"./results/\" + str(lam) + \"_logfile.csv\"\n",
    "        copy_u = np.copy(u)\n",
    "        train_model(copy_u, lam, l_rate, train_data, epochs=50, steps=300, ex_per_epoch=50, log=True, log_filename=filename, eval_data=eval_data)\n",
    "\n",
    "def final_eval(lam, train_data, test_data):\n",
    "    u = np.random.rand(7,1)\n",
    "    optimized_u = np.array(train_model(u, lam, l_rate, train_data)[0]).reshape(7,1)\n",
    "    submission = open(\"./results/submission.csv\", \"a+\")\n",
    "    #submission.write(\"Example,Label\\n\")\n",
    "    for i in range(0, len(test_data)):\n",
    "        features = np.array(test_data[i]).reshape(6,1)\n",
    "        prediction = classify(optimized_u, features)\n",
    "        if prediction > 0:\n",
    "            # >50K\n",
    "            #submission.write(\"'\"+ str(i) +\"'\" + \",>50K\\n\")\n",
    "            submission.write(\">50K\\n\")\n",
    "        else:\n",
    "            #\" <=50K\"\n",
    "            submission.write(\"<=50K\\n\")\n",
    "    submission.close()\n",
    "\n",
    "def main():\n",
    "    #   Retrive Data\n",
    "    scaled_train_data = np.array(get_data(\"data/processed/scaled_train_and_label_data.csv\")).astype(float)\n",
    "    scaled_final_test_data = np.array(get_data(\"data/processed/scaled_test_data.csv\")).astype(float)\n",
    "\n",
    "    #   Split Data\n",
    "    splitted_train_data = split_data(scaled_train_data)\n",
    "    master_train_data = splitted_train_data[1]\n",
    "    master_eval_data = splitted_train_data[0]\n",
    "\n",
    "    #   Probes Lambda Values and prints respective accuracies into a file\n",
    "    #       File format: all lambdas are listed, then all accuracies are listed\n",
    "    #           Yes I know the format is pretty bad... but it works\n",
    "    # lambdas_to_probe = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "    # accuracies = np.array(probe_lambdas(lambdas_to_probe, l_rate, master_train_data))\n",
    "    # print(accuracies)\n",
    "    # accuracies_file = np.append(np.array(lambdas_to_probe), accuracies)\n",
    "    # exportDataToCSV(\"./results/lambdas_accuracies.csv\", accuracies_file)\n",
    "\n",
    "    #   Log Accuracy and Norm of Parameters for each lambda value\n",
    "    #log_lambdas(lambdas_to_probe, l_rate, master_train_data)\n",
    "\n",
    "    #   Create Kaggle Submissione, train using all data and best lambda value\n",
    "    lam = 1e-5\n",
    "    final_eval(lam, scaled_train_data, scaled_final_test_data)\n",
    "\n",
    "\n",
    "\n",
    "#   Create a Log File\n",
    "def logger(step, filename, u, lam, eval_data):\n",
    "    log = open(filename, \"a+\")\n",
    "    accuracy = evaluate_model(u, lam, eval_data)\n",
    "    magnitude = np.linalg.norm(u)\n",
    "    message = str(step) + \",\" + str(accuracy) + \",\" + str(magnitude) +\"\\n\"\n",
    "    log.write(message)\n",
    "    log.close()\n",
    "\n",
    "def get_data(path):\n",
    "    data = []\n",
    "    with open(path, 'r') as csvfile:\n",
    "        raw_data = csv.reader(csvfile, delimiter = ',')\n",
    "        i = 0\n",
    "        for row in raw_data:\n",
    "            data.append([])\n",
    "            for element in row:\n",
    "                data[i].append(float(element))\n",
    "            i += 1\n",
    "    return data\n",
    "\n",
    "#   Splits Data into 10% / 90% chunks\n",
    "#   returns : a tuple with the first element being the 10% chunk, second element\n",
    "#       being the 90% chunk\n",
    "def split_data(data):\n",
    "    np.random.shuffle(data)\n",
    "    n = len(data)\n",
    "    chunk_len = n // 10\n",
    "    eval_data = data[0:chunk_len]\n",
    "    train_data = data[chunk_len: n]\n",
    "    return (eval_data, train_data)\n",
    "\n",
    "def choose_random_data(data, examples):\n",
    "    np.random.shuffle(data)\n",
    "    n = len(data)\n",
    "    return data[0:examples]\n",
    "\n",
    "def printFirst10(data):\n",
    "    for i in range(0, 10):\n",
    "        print(data[i])\n",
    "\n",
    "def exportDataToCSV(filename, data):\n",
    "    np.savetxt(filename, data, delimiter=\",\")\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sushanta/Documents/Kaggle/Illinois/CS 598 - Applied Machine Learning/HW2'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_data = np.array(get_data(\"data/processed/scaled_train_and_label_data.csv\")).astype(float)\n",
    "scaled_final_test_data = np.array(get_data(\"data/processed/scaled_test_data.csv\")).astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43957, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_train_data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
