---
title: 'Data Analysis Project - STAT 420 - Group Project'
author: "Luma Vasiljevic(lumav2), Sushanta Panda(panda5)"
date: 
output:
  html_document:
    theme: flatly
    toc: yes
    fig_width: 10
    fig_height: 5
  pdf_document:
    toc: yes
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
library(knitr)
opts_chunk$set(cache = TRUE, autodep = TRUE)
```

# Introduction


###Description of the dataset:
Forced Expiratory Volume (FEV) is an index of pulmonary function that measures the volume of the air expelled after one second of constant effort. The data contains the determinations of FEB on 654 children ages 6 – 22 who are seen in childhood respiratory disease study in 1980 in East Boston, Massachusetts. The data are part of a larger study to follow the change in pulmonary function over time in children.

#####**Dataset Link: **
http://www.statsci.org/data/general/fev.html

#####**Variables in the Dataset:**

```{r include=FALSE}
Variable = data.frame(
  VariableName = c("ID","Age", "Height", "Sex", "Smoker", "FEV"),
  Category = c("Numeric","Numeric", "Numeric", "Categorical", "Categorical", "Numeric"),
  Description = c("Uniquely Identified Row","Age of the child", "Height of the child in inches", "Sex of the child ", "Whether the child is a non-smoker or current smoker", "FEV of the child in litres")
)
```


```{r echo=FALSE}
library(knitr)
kable(Variable, format = "pandoc",padding = 2)
```

#####**Background information of the dataset:**

The data contains the determinations of FEB on 654 children ages 6 – 22 who are seen in childhood respiratory disease study in 1980 in East Boston, Massachusetts. The data are part of a larger study to follow the change in pulmonary function over time in children

Note: No citation required for this source (http://www.statsci.org/data/general/fev.html)


###Why is it interesting:
This dataset has chosen by us because of personal interests, whether we can predict the child’s FEV with the help of the available predictor, rather going for a Pulmonary Function Test, which will identify any pulmonary disease of a child. And secondly, to do statistical analysis on what are the predictors responsible to increase / decrease the pulmonary function of the child and try to find answer on these lines.

###Why are we creating a model of this Data:
In order to **predict the child's Forced Exporatory Volume (FEV)** based on the Child's Age, Height, Sex and whether the child is a smoker or not. This will help for kick-start the treatment of the child based on the FEV reading, rather going throgh the Pulmonary Function test.


###Goal of the Model:
The goal of the model is to find the best model after going through the several methods, which predict the child's FEV with minimal error. The best model would have the lowest Root Mean Square Error (RMSE) against Leave One Out cross validation (LOOCV).


# Methods
We will be doing several data analysis and methods in this section, where each section will be describing what's the part of the tasks.

###Load FEV data, Observations:

```{r}
childfev = read.csv("http://www.statsci.org/data/general/fev.txt",
                    sep = "\t",
                    quote = "\"",
                    comment.char = "",
                    stringsAsFactors = FALSE)
childfev$Sex = as.factor(childfev$Sex)
childfev$Smoker = as.factor(childfev$Smoker)
str(childfev)
dim(childfev)
head(childfev$FEV,10)
```

From the dataset, it observered data, Age, Heigh is a numerical variable, where as Sex / Smoker field is a categorical variable. The FEV is the numerical **response** variable.


#####**Load FEV data:**
```{r fig.height=10, fig.width=10}
pairs(childfev[c('Age','FEV','Height','Sex','Smoker')], col = "darkgrey")
```

From the pairs plot, it sees there is a clear signs of linear releationship between FEV and Height. However there is no clear sign of linear relationship between FEV and Age variable. The 2 categorical variable `Sex` and `Smoker` seems to have 2 distinct data. Let's explore this

**Let's check the Correlation Matrix to explore what's the co-relation among the variables**
```{r}
cor(childfev[c('Age','Height','FEV')])
```
It seems that what the pair plot shows seems correct, from the corelation matrix it also suggests that `Age` and `Height` are higly corelation with `FEV` response, as well as among it self,  We need to explore the variance inflation factor (VIF) while creating our model in further.


#####**Categorical Data of Sex and Smoker**
```{r}
table(childfev$Sex)
table(childfev$Smoker)
```

###MLR FEV data:
Let's start with the Multiple Linear Regression (MLR) against all the predictor (Except the ID), to predict the FEV as the response

```{r}
mlr_model = lm(FEV ~ Age + Height + Sex + Smoker, data = childfev)
summary(mlr_model)
```

#####**Assumption of the Model**
Let's verify the Assumption of the Model
 - Constant Varience
 - Normality 

**Let's plot Plotted Versus Residul Plot to see the distribution**
```{r}
par(mfrow = c(1,2))
plot(resid(mlr_model)~fitted(mlr_model), ylab = "residuals", xlab = "fitted", cex = 1, pch = 1, col = "darkgrey", main = "Residuals Versis Fitted Model")
abline(h = 0, lwd = 2, col = "darkorange")

qqnorm(resid(mlr_model), cex = 1, pch = 1, col = "darkgrey")
qqline(resid(mlr_model), lwd = 2, col = "darkorange")
```

From the above residuals versus fitted plot, it seems the **constant variance assumption is suspect**, as the residuals are not equally distributed. 

This is the same case for the Q-Q plot, where both the tail seems to the a little fat tails, which tells us that **normality assuption seems suspect** for the model.

Let's do the BP Test and Shapiro Test to gather some more evidence

```{r warning=FALSE}
bptest(mlr_model)
```

The BP Test is very low which confirms that **constant variance is suspect**

```{r}
shapiro.test(resid(mlr_model))
```

The shapiro test also seems lkow, which confirms that the **normality assumption also seems suspect**

#####**RSS, RMSE and RMSE LOOCV**
Let'e calculate the RSS, RMSE and RMSE LOOCV of the model
**RSS**
```{r}
sum((resid(mlr_model))^2)
```

**RMSE**
```{r}
sqrt(mean(sum((resid(mlr_model))^2)))
```

**RMSE LOOCV**
```{r}
sqrt(mean((resid(mlr_model)/(1-hatvalues(mlr_model)))^2))
```

```{r include=FALSE}
df_mlr_model = data.frame(
  Metric = c("RSS","RMSE", "RMSE LOOCV"),
  Value = c(sum((resid(mlr_model))^2),sqrt(mean(sum((resid(mlr_model))^2))),sqrt(mean((resid(mlr_model)/(1-hatvalues(mlr_model)))^2)))
)
```

```{r echo=FALSE}
library(knitr)
kable(mlr_model, format = "pandoc",padding = 2)
```

###Transformation of FEV (Logarithimic):
Let's explore whether transformation of the response variable `FEV` has any siginificance to improve the score (low RMSE LOOCV)


Let's first plot the histogram of the `FEV` response variable with and without the logarathimic value

```{r}
par(mfrow=c(1,2))
hist(childfev$FEV,
     xlab = 'FEV response Variable Data',
     probability = TRUE,
     breaks = 20,
     col = "darkgrey",
     name = "Original FEV response")

hist(log(childfev$FEV),
     xlab = 'log(FEV) response Variable Data',     
     breaks = 20,
     col = "darkgrey",
     name = "Logarithimic FEV response")
```

It seems that after the logarithimic transformation the `FEV` response doesn't seems to be normal distibution. Hence let's not explore the lograthimic transformation for the `FEV` response variable

###Transformation of FEV (Polynomial):
Let's explore the polynomial transformation of the predictor to see whether we can able to find the best model which lower the RMSE LOOCV

#####**quadriatic Transformation of the Numerical Predictors:**

```{r}
quad_model = lm(FEV ~ Sex + Smoker + poly(Age,2) + poly(Height,2), data = childfev)
summary(quad_model)
```

It seems the quadratic model is significance with low p value **<2e-16**, including all the predictors are significance though except `Age` where the p value is around `r summary(quad_model)$coefficientp[2,4]`

Let's check the Assumption of the model

#####**Assumption of the Model**
Let's verify the Assumption of the Model
 - Constant Varience
 - Normality 

**Let's plot Plotted Versus Residul Plot to see the distribution**
```{r}
par(mfrow = c(1,2))
plot(resid(quad_model)~fitted(quad_model), ylab = "residuals", xlab = "fitted", cex = 1, pch = 1, col = "darkgrey", main = "Residuals Versis Fitted Model - Quad")
abline(h = 0, lwd = 2, col = "darkorange")

qqnorm(resid(quad_model), cex = 1, pch = 1, col = "darkgrey")
qqline(resid(quad_model), lwd = 2, col = "darkorange")
```

From the above residuals versus fitted plot, though it seems better than that of **Multiple Linear Regression**, it's not doing good job for the higher fitted values. It seems as the fitted value goes higher, the residual seems to be wider, hence the **constant variance assumption is suspect**

This is the same case for the Q-Q plot, where both the tail seems to the a little fat tails, which tells us that **normality assuption seems suspect** for the model.

Let's do the BP Test and Shapiro Test to gather some more evidence

```{r warning=FALSE}
bptest(quad_model)
```

The BP Test is very low which confirms that **constant variance is suspect**

```{r}
shapiro.test(resid(quad_model))
```

The shapiro test also seems lkow, which confirms that the **normality assumption also seems suspect**

#####**RSS, RMSE and RMSE LOOCV**
Let'e calculate the RSS, RMSE and RMSE LOOCV of the model
**RSS**
```{r}
sum((resid(quad_model))^2)
```

**RMSE**
```{r}
sqrt(mean(sum((resid(quad_model))^2)))
```

**RMSE LOOCV**
```{r}
sqrt(mean((resid(quad_model)/(1-hatvalues(quad_model)))^2))
```

```{r include=FALSE}
df_quad_model = data.frame(
  Metric = c("RSS","RMSE", "RMSE LOOCV"),
  Value = c(sum((resid(quad_model))^2),sqrt(mean(sum((resid(quad_model))^2))),sqrt(mean((resid(quad_model)/(1-hatvalues(quad_model)))^2)))
)
```

```{r echo=FALSE}
library(knitr)
kable(df_quad_model, format = "pandoc",padding = 2)
```
The RMSE LOOCV seems better than that of `Multiple Linear Regression`. We will compare at the last while making the decission around models



#####**Cubic Transformation of the Numerical Predictors:**

```{r}
cube_model = lm(FEV ~ Sex + Smoker + poly(Age,3) + poly(Height,3), data = childfev)
summary(cube_model)
```

It seems the quadratic model is significance with low p value **<2e-16**, including all the predictors are significance though except `Age` where the p value is around `r summary(cube_model)$coefficientp[2,4]`

Let's check the Assumption of the model

#####**Assumption of the Model**
Let's verify the Assumption of the Model
 - Constant Varience
 - Normality 

**Let's plot Plotted Versus Residul Plot to see the distribution**
```{r}
par(mfrow = c(1,2))
plot(resid(cube_model)~fitted(cube_model), ylab = "residuals", xlab = "fitted", cex = 1, pch = 1, col = "darkgrey", main = "Residuals Versis Fitted Model - Cube")
abline(h = 0, lwd = 2, col = "darkorange")

qqnorm(resid(cube_model), cex = 1, pch = 1, col = "darkgrey")
qqline(resid(cube_model), lwd = 2, col = "darkorange")
```

From the above residuals versus fitted plot, though it seems better than that of **Multiple Linear Regression**, it's not doing good job for the higher fitted values. It seems as the fitted value goes higher, the residual seems to be wider, hence the **constant variance assumption is suspect**

This is the same case for the Q-Q plot, where both the tail seems to the a little fat tails, which tells us that **normality assuption seems suspect** for the model.

Let's do the BP Test and Shapiro Test to gather some more evidence

```{r warning=FALSE}
bptest(cube_model)
```

The BP Test is very low which confirms that **constant variance is suspect**

```{r}
shapiro.test(resid(cube_model))
```

The shapiro test also seems lkow, which confirms that the **normality assumption also seems suspect**

#####**RSS, RMSE and RMSE LOOCV**
Let'e calculate the RSS, RMSE and RMSE LOOCV of the model
**RSS**
```{r}
sum((resid(cube_model))^2)
```

**RMSE**
```{r}
sqrt(mean(sum((resid(cube_model))^2)))
```

**RMSE LOOCV**
```{r}
sqrt(mean((resid(cube_model)/(1-hatvalues(cube_model)))^2))
```

```{r include=FALSE}
df_cube_model = data.frame(
  Metric = c("RSS","RMSE", "RMSE LOOCV"),
  Value = c(sum((resid(cube_model))^2),sqrt(mean(sum((resid(cube_model))^2))),sqrt(mean((resid(cube_model)/(1-hatvalues(cube_model)))^2)))
)
```

```{r echo=FALSE}
library(knitr)
kable(df_cube_model, format = "pandoc",padding = 2)
```
The RMSE LOOCV seems better than that of `Multiple Linear Regression`. We will compare at the last while making the decission around models


#####**Higher Order Transformation of the Numerical Predictors:**

```{r}
high_model = lm(FEV ~ Sex + Smoker + poly(Age,6) + poly(Height,6), data = childfev)
summary(high_model)
```

It seems the quadratic model is significance with low p value **<2e-16**, including all the predictors are significance though except `Age` where the p value is around `r summary(high_model)$coefficientp[2,4]`

Let's check the Assumption of the model

#####**Assumption of the Model**
Let's verify the Assumption of the Model
 - Constant Varience
 - Normality 

**Let's plot Plotted Versus Residul Plot to see the distribution**
```{r}
par(mfrow = c(1,2))
plot(resid(high_model)~fitted(high_model), ylab = "residuals", xlab = "fitted", cex = 1, pch = 1, col = "darkgrey", main = "Residuals Versis Fitted Model - higher order Polynomial")
abline(h = 0, lwd = 2, col = "darkorange")

qqnorm(resid(high_model), cex = 1, pch = 1, col = "darkgrey")
qqline(resid(high_model), lwd = 2, col = "darkorange")
```

From the above residuals versus fitted plot, though it seems better than that of **Multiple Linear Regression**, it's not doing good job for the higher fitted values. It seems as the fitted value goes higher, the residual seems to be wider, hence the **constant variance assumption is suspect**

This is the same case for the Q-Q plot, where both the tail seems to the a little fat tails, which tells us that **normality assuption seems suspect** for the model.

Let's do the BP Test and Shapiro Test to gather some more evidence

```{r warning=FALSE}
bptest(high_model)
```

The BP Test is very low which confirms that **constant variance is suspect**

```{r}
shapiro.test(resid(high_model))
```

The shapiro test also seems lkow, which confirms that the **normality assumption also seems suspect**

#####**RSS, RMSE and RMSE LOOCV**
Let'e calculate the RSS, RMSE and RMSE LOOCV of the model
**RSS**
```{r}
sum((resid(high_model))^2)
```

**RMSE**
```{r}
sqrt(mean(sum((resid(high_model))^2)))
```

**RMSE LOOCV**
```{r}
sqrt(mean((resid(high_model)/(1-hatvalues(high_model)))^2))
```

```{r include=FALSE}
df_high_model = data.frame(
  Metric = c("RSS","RMSE", "RMSE LOOCV"),
  Value = c(sum((resid(high_model))^2),sqrt(mean(sum((resid(high_model))^2))),sqrt(mean((resid(high_model)/(1-hatvalues(high_model)))^2)))
)
```

```{r echo=FALSE}
library(knitr)
kable(df_high_model, format = "pandoc",padding = 2)
```

It seems that the model is not improving beyong degree 2 and infact it's detorating after degree 2. We will explore the polynoamial with the interaction to see if there is any improvement. Also will create a bigger model and by the help of AIC and BIC to explore, whether we can improve the model.








###Interaction between Predictors:
Let's explore the Interaction between the predictor to see whether we can able to find the best model which lower the RMSE LOOCV

#####**2 way Interaction:**

```{r}
two_int_model = lm(FEV ~ (Age + Height + Sex + Smoker) ^ 2, data = childfev)
summary(two_int_model)
```

It seems the quadratic model is significance with low p value **<2e-16**, including all the predictors are significance though except `Age` where the p value is around `r summary(two_int_model)$coefficientp[2,4]`

Let's check the Assumption of the model

#####**Assumption of the Model**
Let's verify the Assumption of the Model
 - Constant Varience
 - Normality 

**Let's plot Plotted Versus Residul Plot to see the distribution**
```{r}
par(mfrow = c(1,2))
plot(resid(two_int_model)~fitted(two_int_model), ylab = "residuals", xlab = "fitted", cex = 1, pch = 1, col = "darkgrey", main = "Residuals Versis Fitted Model - Quad")
abline(h = 0, lwd = 2, col = "darkorange")

qqnorm(resid(two_int_model), cex = 1, pch = 1, col = "darkgrey")
qqline(resid(two_int_model), lwd = 2, col = "darkorange")
```

From the above residuals versus fitted plot, though it seems better than that of **Multiple Linear Regression**, it's not doing good job for the higher fitted values. It seems as the fitted value goes higher, the residual seems to be wider, hence the **constant variance assumption is suspect**

This is the same case for the Q-Q plot, where both the tail seems to the a little fat tails, which tells us that **normality assuption seems suspect** for the model.

Let's do the BP Test and Shapiro Test to gather some more evidence

```{r warning=FALSE}
bptest(two_int_model)
```

The BP Test is very low which confirms that **constant variance is suspect**

```{r}
shapiro.test(resid(two_int_model))
```

The shapiro test also seems lkow, which confirms that the **normality assumption also seems suspect**

#####**RSS, RMSE and RMSE LOOCV**
Let'e calculate the RSS, RMSE and RMSE LOOCV of the model
**RSS**
```{r}
sum((resid(two_int_model))^2)
```

**RMSE**
```{r}
sqrt(mean(sum((resid(two_int_model))^2)))
```

**RMSE LOOCV**
```{r}
sqrt(mean((resid(two_int_model)/(1-hatvalues(two_int_model)))^2))
```

```{r include=FALSE}
df_two_int_model = data.frame(
  Metric = c("RSS","RMSE", "RMSE LOOCV"),
  Value = c(sum((resid(two_int_model))^2),sqrt(mean(sum((resid(two_int_model))^2))),sqrt(mean((resid(two_int_model)/(1-hatvalues(two_int_model)))^2)))
)
```

```{r echo=FALSE}
library(knitr)
kable(df_two_int_model, format = "pandoc",padding = 2)
```

The RMSE LOOCV seems better than that of `Multiple Linear Regression`. We will compare at the last while making the decission around models








#####**3 way Interaction:**

```{r}
three_int_model = lm(FEV ~ (Age + Height + Sex + Smoker) ^ 3, data = childfev)
summary(three_int_model)
```

It seems the quadratic model is significance with low p value **<2e-16**, including all the predictors are significance though except `Age` where the p value is around `r summary(three_int_model)$coefficientp[2,4]`

Let's check the Assumption of the model

#####**Assumption of the Model**
Let's verify the Assumption of the Model
 - Constant Varience
 - Normality 

**Let's plot Plotted Versus Residul Plot to see the distribution**
```{r}
par(mfrow = c(1,2))
plot(resid(three_int_model)~fitted(three_int_model), ylab = "residuals", xlab = "fitted", cex = 1, pch = 1, col = "darkgrey", main = "Residuals Versis Fitted Model - Quad")
abline(h = 0, lwd = 2, col = "darkorange")

qqnorm(resid(three_int_model), cex = 1, pch = 1, col = "darkgrey")
qqline(resid(three_int_model), lwd = 2, col = "darkorange")
```

From the above residuals versus fitted plot, though it seems better than that of **Multiple Linear Regression**, it's not doing good job for the higher fitted values. It seems as the fitted value goes higher, the residual seems to be wider, hence the **constant variance assumption is suspect**

This is the same case for the Q-Q plot, where both the tail seems to the a little fat tails, which tells us that **normality assuption seems suspect** for the model.

Let's do the BP Test and Shapiro Test to gather some more evidence

```{r warning=FALSE}
bptest(three_int_model)
```

The BP Test is very low which confirms that **constant variance is suspect**

```{r}
shapiro.test(resid(three_int_model))
```

The shapiro test also seems lkow, which confirms that the **normality assumption also seems suspect**

#####**RSS, RMSE and RMSE LOOCV**
Let'e calculate the RSS, RMSE and RMSE LOOCV of the model
**RSS**
```{r}
sum((resid(three_int_model))^2)
```

**RMSE**
```{r}
sqrt(mean(sum((resid(three_int_model))^2)))
```

**RMSE LOOCV**
```{r}
sqrt(mean((resid(three_int_model)/(1-hatvalues(three_int_model)))^2))
```

```{r include=FALSE}
df_three_int_model = data.frame(
  Metric = c("RSS","RMSE", "RMSE LOOCV"),
  Value = c(sum((resid(three_int_model))^2),sqrt(mean(sum((resid(three_int_model))^2))),sqrt(mean((resid(three_int_model)/(1-hatvalues(three_int_model)))^2)))
)
```

```{r echo=FALSE}
library(knitr)
kable(df_three_int_model, format = "pandoc",padding = 2)
```

The RMSE LOOCV seems better than that of `Multiple Linear Regression`. We will compare at the last while making the decission around models


#####**4 way Interaction:**

```{r}
four_int_model = lm(FEV ~ (Age + Height + Sex + Smoker) ^ 4, data = childfev)
summary(four_int_model)
```

It seems the quadratic model is significance with low p value **<2e-16**, including all the predictors are significance though except `Age` where the p value is around `r summary(four_int_model)$coefficientp[2,4]`

Let's check the Assumption of the model

#####**Assumption of the Model**
Let's verify the Assumption of the Model
 - Constant Varience
 - Normality 

**Let's plot Plotted Versus Residul Plot to see the distribution**
```{r}
par(mfrow = c(1,2))
plot(resid(four_int_model)~fitted(four_int_model), ylab = "residuals", xlab = "fitted", cex = 1, pch = 1, col = "darkgrey", main = "Residuals Versis Fitted Model - Quad")
abline(h = 0, lwd = 2, col = "darkorange")

qqnorm(resid(four_int_model), cex = 1, pch = 1, col = "darkgrey")
qqline(resid(four_int_model), lwd = 2, col = "darkorange")
```

From the above residuals versus fitted plot, though it seems better than that of **Multiple Linear Regression**, it's not doing good job for the higher fitted values. It seems as the fitted value goes higher, the residual seems to be wider, hence the **constant variance assumption is suspect**

This is the same case for the Q-Q plot, where both the tail seems to the a little fat tails, which tells us that **normality assuption seems suspect** for the model.

Let's do the BP Test and Shapiro Test to gather some more evidence

```{r warning=FALSE}
bptest(four_int_model)
```

The BP Test is very low which confirms that **constant variance is suspect**

```{r}
shapiro.test(resid(four_int_model))
```

The shapiro test also seems lkow, which confirms that the **normality assumption also seems suspect**

#####**RSS, RMSE and RMSE LOOCV**
Let'e calculate the RSS, RMSE and RMSE LOOCV of the model
**RSS**
```{r}
sum((resid(four_int_model))^2)
```

**RMSE**
```{r}
sqrt(mean(sum((resid(four_int_model))^2)))
```

**RMSE LOOCV**
```{r}
sqrt(mean((resid(four_int_model)/(1-hatvalues(four_int_model)))^2))
```

```{r include=FALSE}
df_four_int_model = data.frame(
  Metric = c("RSS","RMSE", "RMSE LOOCV"),
  Value = c(sum((resid(four_int_model))^2),sqrt(mean(sum((resid(four_int_model))^2))),sqrt(mean((resid(four_int_model)/(1-hatvalues(four_int_model)))^2)))
)
```

```{r echo=FALSE}
library(knitr)
kable(df_four_int_model, format = "pandoc",padding = 2)
```

The RMSE LOOCV seems better than that of `Multiple Linear Regression`. We will compare at the last while making the decission around models







###Model- Big Model, Stepwise via AIC / BIC:
Let's create a big model, with Polynomial of degree 3 and 3 way interaction between categorical-to-categorical, categorical-to-numeric, numeric-to-numeric and see it's score. Also reduced the model via AIC and BIC to see any improvement in the score

#####**Big Model:**

```{r}
big_model = lm(FEV ~ (Age + Height + Sex + Smoker) ^ 3 + poly(Age,3) + poly(Height,3), data = childfev)
summary(big_model)
```

It seems the quadratic model is significance with low p value **<2e-16**, including all the predictors are significance though except `Age` where the p value is around `r summary(big_model)$coefficientp[2,4]`

Let's check the Assumption of the model

#####**Assumption of the Model**
Let's verify the Assumption of the Model
 - Constant Varience
 - Normality 

**Let's plot Plotted Versus Residul Plot to see the distribution**
```{r}
par(mfrow = c(1,2))
plot(resid(big_model)~fitted(big_model), ylab = "residuals", xlab = "fitted", cex = 1, pch = 1, col = "darkgrey", main = "Residuals Versis Fitted Model - Quad")
abline(h = 0, lwd = 2, col = "darkorange")

qqnorm(resid(big_model), cex = 1, pch = 1, col = "darkgrey")
qqline(resid(big_model), lwd = 2, col = "darkorange")
```

From the above residuals versus fitted plot, though it seems better than that of **Multiple Linear Regression**, it's not doing good job for the higher fitted values. It seems as the fitted value goes higher, the residual seems to be wider, hence the **constant variance assumption is suspect**

This is the same case for the Q-Q plot, where both the tail seems to the a little fat tails, which tells us that **normality assuption seems suspect** for the model.

Let's do the BP Test and Shapiro Test to gather some more evidence

```{r warning=FALSE}
bptest(big_model)
```

The BP Test is very low which confirms that **constant variance is suspect**

```{r}
shapiro.test(resid(big_model))
```

The shapiro test also seems lkow, which confirms that the **normality assumption also seems suspect**

#####**RSS, RMSE and RMSE LOOCV**
Let'e calculate the RSS, RMSE and RMSE LOOCV of the model
**RSS**
```{r}
sum((resid(big_model))^2)
```

**RMSE**
```{r}
sqrt(mean(sum((resid(big_model))^2)))
```

**RMSE LOOCV**
```{r}
sqrt(mean((resid(big_model)/(1-hatvalues(big_model)))^2))
```

```{r include=FALSE}
df_big_model = data.frame(
  Metric = c("RSS","RMSE", "RMSE LOOCV"),
  Value = c(sum((resid(big_model))^2),sqrt(mean(sum((resid(big_model))^2))),sqrt(mean((resid(big_model)/(1-hatvalues(big_model)))^2)))
)
```

```{r echo=FALSE}
library(knitr)
kable(df_big_model, format = "pandoc",padding = 2)
```

The RMSE LOOCV seems better than that of `Multiple Linear Regression`. We will compare at the last while making the decission around models




#####**Reduced Big Model via AIC :**

```{r}
big_model = lm(FEV ~ (Age + Height + Sex + Smoker) ^ 3 + poly(Age,3) + poly(Height,3), data = childfev)
big_model_aic = step(big_model, direction = "backward", trace = 0)
summary(big_model_aic)
```

It seems the quadratic model is significance with low p value **<2e-16**, including all the predictors are significance though except `Age` where the p value is around `r summary(big_model_aic)$coefficientp[2,4]`

Let's check the Assumption of the model

#####**Assumption of the Model**
Let's verify the Assumption of the Model
 - Constant Varience
 - Normality 

**Let's plot Plotted Versus Residul Plot to see the distribution**
```{r}
par(mfrow = c(1,2))
plot(resid(big_model_aic)~fitted(big_model_aic), ylab = "residuals", xlab = "fitted", cex = 1, pch = 1, col = "darkgrey", main = "Residuals Versis Fitted Model - Quad")
abline(h = 0, lwd = 2, col = "darkorange")

qqnorm(resid(big_model_aic), cex = 1, pch = 1, col = "darkgrey")
qqline(resid(big_model_aic), lwd = 2, col = "darkorange")
```

From the above residuals versus fitted plot, though it seems better than that of **Multiple Linear Regression**, it's not doing good job for the higher fitted values. It seems as the fitted value goes higher, the residual seems to be wider, hence the **constant variance assumption is suspect**

This is the same case for the Q-Q plot, where both the tail seems to the a little fat tails, which tells us that **normality assuption seems suspect** for the model.

Let's do the BP Test and Shapiro Test to gather some more evidence

```{r warning=FALSE}
bptest(big_model_aic)
```

The BP Test is very low which confirms that **constant variance is suspect**

```{r}
shapiro.test(resid(big_model_aic))
```

The shapiro test also seems lkow, which confirms that the **normality assumption also seems suspect**

#####**RSS, RMSE and RMSE LOOCV**
Let'e calculate the RSS, RMSE and RMSE LOOCV of the model
**RSS**
```{r}
sum((resid(big_model_aic))^2)
```

**RMSE**
```{r}
sqrt(mean(sum((resid(big_model_aic))^2)))
```

**RMSE LOOCV**
```{r}
sqrt(mean((resid(big_model_aic)/(1-hatvalues(big_model_aic)))^2))
```

```{r include=FALSE}
df_big_model_aic = data.frame(
  Metric = c("RSS","RMSE", "RMSE LOOCV"),
  Value = c(sum((resid(big_model_aic))^2),sqrt(mean(sum((resid(big_model_aic))^2))),sqrt(mean((resid(big_model_aic)/(1-hatvalues(big_model_aic)))^2)))
)
```

```{r echo=FALSE}
library(knitr)
kable(df_big_model_aic, format = "pandoc",padding = 2)
```

The RMSE LOOCV seems better than that of `Multiple Linear Regression`. We will compare at the last while making the decission around models


###Compare RSS, RMSE, RMSE LOOCV - All Models:

```{r}
compare_model = data.frame(
  Model = c("mlr","quad", "cube", "high_poly", "i2_way_int","i3_way_int","i4_way_int","big_model","big_model_aic"),
  No_Of_Parameters = c(length(coef(mlr_model)),length(coef(quad_model)),length(coef(cube_model)),length(coef(high_model)),length(coef(two_int_model)),length(coef(three_int_model)),length(coef(four_int_model)),length(coef(big_model)),length(coef(big_model_aic))),
  RSS = c(sum((resid(mlr_model))^2),sum((resid(quad_model))^2),sum((resid(cube_model))^2),sum((resid(high_model))^2),sum((resid(two_int_model))^2),sum((resid(three_int_model))^2),sum((resid(four_int_model))^2),sum((resid(big_model))^2),sum((resid(big_model_aic))^2)),
  RMSE = c(sqrt(mean(sum((resid(mlr_model))^2))),sqrt(mean(sum((resid(quad_model))^2))),sqrt(mean(sum((resid(cube_model))^2))),sqrt(mean(sum((resid(high_model))^2))),sqrt(mean(sum((resid(two_int_model))^2))),sqrt(mean(sum((resid(three_int_model))^2))),sqrt(mean(sum((resid(four_int_model))^2))),sqrt(mean(sum((resid(big_model))^2))),sqrt(mean(sum((resid(big_model_aic))^2)))),
  RMSE_LOOCV = c(sqrt(mean((resid(mlr_model)/(1-hatvalues(mlr_model)))^2)),sqrt(mean((resid(quad_model)/(1-hatvalues(quad_model)))^2)),sqrt(mean((resid(cube_model)/(1-hatvalues(cube_model)))^2)),sqrt(mean((resid(high_model)/(1-hatvalues(high_model)))^2)),sqrt(mean((resid(two_int_model)/(1-hatvalues(two_int_model)))^2)),sqrt(mean((resid(three_int_model)/(1-hatvalues(three_int_model)))^2)),sqrt(mean((resid(four_int_model)/(1-hatvalues(four_int_model)))^2)),sqrt(mean((resid(big_model)/(1-hatvalues(big_model)))^2)),sqrt(mean((resid(big_model_aic)/(1-hatvalues(big_model_aic)))^2)))  
  )
```



```{r echo=FALSE}
library(knitr)
kable(compare_model, format = "pandoc",padding = 2)
```
