---
title: "Week 10 - Homework"
author: "STAT 420, Summer 2019, Sushanta Panda"
date: ''
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
```

## Exercise 1 (Simulating Wald and Likelihood Ratio Tests)

In this exercise we will investigate the distributions of hypothesis tests for logistic regression. For this exercise, we will use the following predictors.

```{r}
sample_size = 150
set.seed(420)
x1 = rnorm(n = sample_size)
x2 = rnorm(n = sample_size)
x3 = rnorm(n = sample_size)
```

Recall that

$$
p({\bf x}) = P[Y = 1 \mid {\bf X} = {\bf x}]
$$

Consider the true model

$$
\log\left(\frac{p({\bf x})}{1 - p({\bf x})}\right) = \beta_0 + \beta_1 x_1
$$

where

- $\beta_0 = 0.4$
- $\beta_1 = -0.35$

**(a)** To investigate the distributions, simulate from this model 2500 times. To do so, calculate 

$$
P[Y = 1 \mid {\bf X} = {\bf x}]
$$ 

for an observation, and then make a random draw from a Bernoulli distribution with that success probability. (Note that a Bernoulli distribution is a Binomial distribution with parameter $n = 1$. There is no direction function in `R` for a Bernoulli distribution.)

Each time, fit the model:

$$
\log\left(\frac{p({\bf x})}{1 - p({\bf x})}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3
$$


```{r}
beta_0 = 0.4
beta_1 = -0.35
  
wald_test = rep(0,2500)
liklihood_test = rep(0,2500)
for(i in (1:2500)){
  eta = beta_0 + beta_1 * x1 #simulate from this model
  p = ( 1/(1+exp(-eta)) )
  y = rbinom(n = sample_size, size = 1, p)
  model_full = glm(y ~ x1 + x2 + x3, family = binomial)
  model_x1 = glm(y ~ x1, family = binomial)   
  wald_test[i] = summary(model_full)$coefficient[3,3]
  liklihood_test[i] = anova(model_x1,model_full,test="LRT")[2,4]
}
```


Store the test statistics for two tests:

- The Wald test for $H_0: \beta_2 = 0$, which we say follows a standard normal distribution for "large" samples

    ```{r}
    head(wald_test,10)
    ```

- The likelihood ratio test for $H_0: \beta_2 = \beta_3 = 0$, which we say follows a $\chi^2$ distribution (with some degrees of freedom) for "large" samples

    ```{r}
    head(liklihood_test,10)
    ```

**(b)** Plot a histogram of the empirical values for the Wald test statistic. Overlay the density of the true distribution assuming a large sample.

```{r}
sample_data = seq(-4,4,length = 1000)
sample_data_norm = dnorm(sample_data)

x = seq(-4,4,length = 1000)

hist(wald_test,
     probability = TRUE,
     xlim = c(-4,4) , 
     ylim = c(0,0.4) ,
     xlab = "z value" ,
     col="white",
     border="darkgrey",
     lwd = 2,
     main = "Empirical Distribution of Wald Test"
     )

curve(dnorm(x),
      lwd=2,
      col = "dodgerblue",
      add = TRUE)
```


**(c)** Use the empirical results for the Wald test statistic to estimate the probability of observing a test statistic larger than 1. Also report this probability using the true distribution of the test statistic assuming a large sample.


** **
```{r}
mean (wald_test > 1)
```

The empirical results for the Wald test statistic to estimate the probability of observing a test statistic larger than 1 is **`r mean (wald_test > 1)`**

```{r}
pnorm(1, mean = 0, sd = 1, lower.tail = FALSE)
```

The probability using the true distribution of the **wald test** statistic assuming a large sample is : **`r pnorm(1, mean = 0, sd = 1, lower.tail = FALSE)`**


**(d)** Plot a histogram of the empirical values for the likelihood ratio test statistic. Overlay the density of the true distribution assuming a large sample.

```{r}
x = seq(-4, 4 , length = 1000)

hist(liklihood_test,
     xlim = c(-1,12) ,
     xlab = "Likelihood Ratio Test" ,
     ylim = c(0,0.5) ,
     probability = TRUE,
     col="white",
     border="darkgrey",
     lwd = 2,
     main = "Empirical Distribution of Likelihood Ratio Test"
     )

curve(dchisq(x, df = 2),
      lwd=2,
      col = "dodgerblue",
      add = TRUE)
```

**(e)** Use the empirical results for the likelihood ratio test statistic to estimate the probability of observing a test statistic larger than 5. Also report this probability using the true distribution of the test statistic assuming a large sample.


```{r}
mean (liklihood_test > 5)
```

The empirical results for the **likelihood ratio test statistic** to estimate the probability of observing a test statistic larger than 5 is **`r pchisq(5, df = 2, ncp = 0, lower.tail = FALSE, log.p = FALSE)`**


```{r}
pchisq(5, df = 2, ncp = 0, lower.tail = FALSE, log.p = FALSE)
```

The probability using the true distribution of the **likelihood ratio test statistic** statistic assuming a large sample is : **`r pchisq(5, df = 2, ncp = 0, lower.tail = FALSE, log.p = FALSE)`**

**(f)** Repeat **(a)**-**(e)** but with simulation using a smaller sample size of 10. Based on these results, is this sample size large enough to use the standard normal and $\chi^2$ distributions in this situation? Explain.

```{r}
sample_size = 10
set.seed(420)
x1 = rnorm(n = sample_size)
x2 = rnorm(n = sample_size)
x3 = rnorm(n = sample_size)
```

**Simulate, for sample size = 10**
```{r warning=FALSE}

beta_0 = 0.4
beta_1 = -0.35
  
wald_test = rep(0,2500)
liklihood_test = rep(0,2500)
for(i in (1:2500)){
  eta = beta_0 + beta_1 * x1 #simulate from this model
  p = ( 1/(1+exp(-eta)) )
  y = rbinom(n = sample_size, size = 1, p)
  model_full = glm(y ~ x1 + x2 + x3, family = binomial)
  model_x1 = glm(y ~ x1, family = binomial)   
  wald_test[i] = summary(model_full)$coefficient[3,3]
  liklihood_test[i] = anova(model_x1,model_full,test="LRT")[2,4]
}
```


Store the test statistics for two tests:

- The Wald test for $H_0: \beta_2 = 0$, which we say follows a standard normal distribution for "small" samples (Size = 10)

    ```{r}
    head(wald_test,10)
    ```

- The likelihood ratio test for $H_0: \beta_2 = \beta_3 = 0$, which we say follows a $\chi^2$ distribution (with some degrees of freedom) for "small" samples (Size = 10)

    ```{r}
    head(liklihood_test,10)
    ```

**Plot a histogram of the empirical values for the Wald test statistic. Overlay the density of the true distribution assuming a small sample (Size = 10)**

```{r}
sample_data = seq(-4,4,length = 1000)
sample_data_norm = dnorm(sample_data)

x = seq(-4,4,length = 1000)

hist(wald_test,
     probability = TRUE,
     xlim = c(-2,2) , 
     #ylim = c(0,0.4) ,
     xlab = "z value" ,
     col="white",
     border="darkgrey",
     lwd = 2,
     main = "Empirical Distribution of Wald Test"
     )

curve(dnorm(x),
      lwd=2,
      col = "dodgerblue",
      add = TRUE)
```


**Comments**

It seems with lower the sample size, the wald test statistics distribution **doesn't follow a normal distribution**. This is because of the fact that, lower the number of sample size is not enough to produce the correct empirical distribution of the wald test distribution.


**Empirical results for the Wald test statistic to estimate the probability of observing a test statistic larger than 1**

```{r}
mean (wald_test > 1)
```

The empirical results for the Wald test statistic to estimate the probability of observing a test statistic larger than 1 is **`r mean (wald_test > 1)`**

**Report the probability using the true distribution of the test statistic assuming a small sample (Size = 10)**

```{r}
pnorm(1, mean = 0, sd = 1, lower.tail = FALSE)
```

The probability using the true distribution of the **wald test** statistic assuming a large sample is : **`r pnorm(1, mean = 0, sd = 1, lower.tail = FALSE)`**


**Plot a histogram of the empirical values for the likelihood ratio test statistic. Overlay the density of the true distribution assuming a small sample (Size = 10)**

```{r}
x = seq(-4, 4 , length = 1000)

hist(liklihood_test,
     xlim = c(-1,12) ,
     xlab = "Likelihood Ratio Test" ,
     ylim = c(0,0.5) ,
     probability = TRUE,
     col="white",
     border="darkgrey",
     lwd = 2,
     main = "Empirical Distribution of Likelihood Ratio Test"
     )

curve(dchisq(x, df = 2),
      lwd=2,
      col = "dodgerblue",
      add = TRUE)
```

**Comments**

In the same way, the Likelihood Ratio test statistics distribution **doesn't follow a Ï‡2 distribution**. This is because of the fact that, lower the number of sample size is not enough to produce the correct empirical distribution of the Likelihood test distribution.


**Empirical results for the likelihood ratio test statistic to estimate the probability of observing a test statistic larger than 5**

```{r}
mean (liklihood_test > 5)
```

The empirical results for the **likelihood ratio test statistic** to estimate the probability of observing a test statistic larger than 5 is **`r pchisq(5, df = 2, ncp = 0, lower.tail = FALSE, log.p = FALSE)`**


**Report this probability using the true distribution of the test statistic assuming a small sample (Size = 10)**

```{r}
pchisq(5, df = 2, ncp = 0, lower.tail = FALSE, log.p = FALSE)
```

The probability using the true distribution of the **likelihood ratio test statistic** statistic assuming a large sample is : **`r pchisq(5, df = 2, ncp = 0, lower.tail = FALSE, log.p = FALSE)`**
 
***

## Exercise 2 (Surviving the Titanic)

For this exercise use the `ptitanic` data from the `rpart.plot` package. (The `rpart.plot` package depends on the `rpart` package.) Use `?rpart.plot::ptitanic` to learn about this dataset. We will use logistic regression to help predict which passengers aboard the [Titanic](https://en.wikipedia.org/wiki/RMS_Titanic) will survive based on various attributes.

```{r, message = FALSE, warning = FALSE}
# install.packages("rpart")
# install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
data("ptitanic")
```

For simplicity, we will remove any observations with missing data. Additionally, we will create a test and train dataset.

```{r}
ptitanic = na.omit(ptitanic)
set.seed(42)
trn_idx = sample(nrow(ptitanic), 300)
ptitanic_trn = ptitanic[trn_idx, ]
ptitanic_tst = ptitanic[-trn_idx, ]
```

**(a)** Consider the model

$$
\log\left(\frac{p({\bf x})}{1 - p({\bf x})}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \beta_5 x_3x_4
$$

where

$$
p({\bf x}) = P[Y = 1 \mid {\bf X} = {\bf x}]
$$

is the probability that a certain passenger survives given their attributes and

- $x_1$ is a dummy variable that takes the value $1$ if a passenger was 2nd class.
- $x_2$ is a dummy variable that takes the value $1$ if a passenger was 3rd class.
- $x_3$ is a dummy variable that takes the value $1$ if a passenger was male.
- $x_4$ is the age in years of a passenger.

Fit this model to the training data and report its deviance.

```{r}
model = glm(survived ~ pclass + sex + age + sex:age, data = ptitanic_trn, family = binomial)
summary(model)
```

The deviance is: **`r summary(model)$deviance`**

**(b)** Use the model fit in **(a)** and an appropriate statistical test to determine if class played a significant role in surviving on the Titanic. Use $\alpha = 0.01$. Report:

- The null hypothesis of the test
$$
H_0 : \beta_1 = \beta_2 = 0
$$

- The test statistic of the test

    Since there are 2 parameters to test the significance and are nested, we will do the **likelihood ratio test statistic** by creating the null and full model as below 

    **Full and null model**
    ```{r}
    full_model = glm(survived ~ pclass + sex + age + sex:age, data = ptitanic_trn, family = binomial)
    full_model
    null_model = glm(survived ~ sex + age + sex:age, data = ptitanic_trn, family = binomial)
    null_model
    ```
    
    **Likelihood Ratio Test Statistics**
    ```{r}
    anova(null_model, full_model, test = "LRT")
    ```

- The p-value of the test

    ```{r}
    anova(null_model, full_model, test = "LRT")[,'Pr(>Chi)'][2]
    ```

    The p-value of the **Likelihood Ratio Test Statistics** is : **`r anova(null_model, full_model, test = "LRT")[,'Pr(>Chi)'][2]`**

- A statistical decision

    ```{r}
    anova(null_model, full_model, test = "LRT")[,'Pr(>Chi)'][2] > 0.01
    ```

    Since the P value of the Likelihood Ratio Test Statistics is  very very small `r anova(null_model, full_model, test = "LRT")[,'Pr(>Chi)'][2]` **< 0.01 ($\alpha$)**, we **reject the null hypothesis**, we reject the hypothesis that $\beta_1 = \beta_2 = 0$

- A practical conclusion

    Since we **reject the null hypothesis** from the Likelihood Ratio Test Statistics, hence class played a **significant role** for surviving people on the Titanic, hence we will keep the class in the model which helps to predict the survival of the people


**(c)** Use the model fit in **(a)** and an appropriate statistical test to determine if an interaction between age and sex played a significant role in surviving on the Titanic. Use $\alpha = 0.01$. Report:

- The null hypothesis of the test

$$
H_0 : \beta_5 = 0
$$

- The test statistic of the test

    Since, it's a single predictor which significance needs to evaluate, we will test the **Wald test** to test the significance of the interaction between age and sex

    ```{r}
    full_model = glm(survived ~ pclass + sex + age + sex:age, data = ptitanic_trn, family = binomial)
    summary(full_model)
    ```

- The p-value of the test

    ```{r}
    summary(full_model)$coefficient['sexmale:age','Pr(>|z|)']
    ```

    The p-value of the **Wald Test** is : **`r summary(full_model)$coefficient['sexmale:age','Pr(>|z|)']`**

- A statistical decision

    Since the p-value from the **wald test** is **`r summary(full_model)$coefficient['sexmale:age','Pr(>|z|)']`** > 0.01 ($\alpha$), we **failed to reject the significance of interaction between age & sex**, means we keep $\beta_5 = 0$

- A practical conclusion

    Since we **failed to reject** $\beta_5 = 0$, the interaction of age and sex is not significance, hence the interaction between Age and Sex is ** not significant role** for surviving people on the Titanic, should excluded from the model


**(d)** Use the model fit in **(a)** as a classifier that seeks to minimize the misclassification rate. Classify each of the passengers in the test dataset. Report the misclassification rate, the sensitivity, and the specificity of this classifier. (Use survived as the positive class.)

**Fitting the Model from point (a)**
```{r}
model = glm(survived ~ pclass + sex + age + sex:age, data = ptitanic_trn, family = binomial)
```

**Clasify each passenger in the PTitanic Test Dataset**
```{r}
test_predict = ifelse(predict(model,ptitanic_tst) > 0 , "survived", "died")
head(test_predict,10)
```

**Mis-classification rate from PTitanic Test Dataset**
```{r}
#testing mis-classification rate
mean(test_predict != ptitanic_tst$survived)
```

The mis-classification rate from the ptitanic test datset is : **`r mean(test_predict != ptitanic_tst$survived)`**

**Function for Confusion matrix, Sensitivity and Specificity**
```{r}
#Confusion matrix creation
make_conf_mat = function(predicted, actual){
  table(predicted = predicted, actual = actual)
}

#Sensitivity Function
get_sens = function(conf_mat){
  conf_mat[2, 2] / sum(conf_mat[, 2])
}

#Specificity Function
get_spec = function(conf_mat){
  conf_mat[1, 1] / sum(conf_mat[, 1])
}
```


**Sensitivity from PTitanic Test Dataset**
```{r}
conf_mat_predict = make_conf_mat(predicted = test_predict, actual = ptitanic_tst$survived)
get_sens(conf_mat_predict)
```

The sensitivity of the Test dataset is : **`r get_sens(conf_mat_predict)`**

**Specificity from PTitanic Test Dataset**
```{r}
conf_mat_predict = make_conf_mat(predicted = test_predict, actual = ptitanic_tst$survived)
get_spec(conf_mat_predict)
```

The specificity of the Test dataset is : **`r get_spec(conf_mat_predict)`**

***

## Exercise 3 (Breast Cancer Detection)

For this exercise we will use data found in [`wisc-train.csv`](wisc-train.csv) and [`wisc-test.csv`](wisc-test.csv), which contain train and test data, respectively. `wisc.csv` is provided but not used. This is a modification of the Breast Cancer Wisconsin (Diagnostic) dataset from the UCI Machine Learning Repository. Only the first 10 feature variables have been provided. (And these are all you should use.)

- [UCI Page](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))
- [Data Detail](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names)

    ```{r}
    wisc_train = read.csv("wisc-train.csv")
    wisc_test = read.csv("wisc-test.csv")
    ```

You should consider coercing the response to be a factor variable if it is not stored as one after importing the data.

**(a)** The response variable `class` has two levels: `M` if a tumor is malignant, and `B` if a tumor is benign. Fit three models to the training data.

- An additive model that uses `radius`, `smoothness`, and `texture` as predictors

    ```{r}
    model1 = glm(class ~ radius + smoothness + texture, data = wisc_train, family = binomial)
    model1
    ```

- An additive model that uses all available predictors

    ```{r}
    model2 = glm(class ~ ., data = wisc_train, family = binomial)
    model2
    ```


- A model chosen via backwards selection using AIC. Use a model that considers all available predictors as well as their two-way interactions for the start of the search.

    ```{r warning=FALSE}
    model = glm(class ~ . + . ^ 2, data = wisc_train, family = binomial)
    model3 = step(model, direction = "backward", trace = 0)
    model3
    ```

For each, obtain a 5-fold cross-validated misclassification rate using the model as a classifier that seeks to minimize the misclassification rate. Based on this, which model is best? Relative to the best, are the other two underfitting or over fitting? Report the test misclassification rate for the model you picked as the best.

```{r warning=FALSE}
#Cross-Validation
library(boot)
set.seed(42)
cv_model1 = cv.glm(wisc_train,model1, K = 5)$delta[1]
cv_model1
set.seed(42)
cv_model2 = cv.glm(wisc_train,model2, K = 5)$delta[1]
cv_model2
set.seed(42)
cv_model3 = cv.glm(wisc_train,model3, K = 5)$delta[1]
cv_model3
```

```{r}
library(knitr)
cv_model = data.frame(
  Model = c("Model 1", "Model 2", "Model 3"),
  Fold_5_CV_Score = c(cv_model1,cv_model2,cv_model3)
)
kable(cv_model, format = "pandoc",padding = 2)
```

From the above, it seems that the best model is **`model 1`**, that is the additive model that uses `radius`, `smoothness`, and `texture` as predictors, because the 5 fold cross validation mis-classification error rate is **`r cv_model1`** (**which is the lowest among the 3 models**)

Relative to the best model (**`model 1`**), it seems both the model (**`model 2`**  and **`model 3`**) are overfitting.  The reason behind is, both the models (**`model 2`**  and **`model 3`**) have higher number of parameters (**`model 2`** number of parameters = **`r length(summary(model2)$coefficient[,1])`**, **`model 3`** number of parameters = **`r length(summary(model3)$coefficient[,1])`**) as compared to the the best model (`Model 1` number of parameter = **`r length(summary(model1)$coefficient[,1])`**), also have high 5 fold cross validation (**`model 2`** 5 fold cv score = `r cv_model2`, **`model 3`** number of parameters = `r cv_model3`) as compared to the best model **`model 1`** whose 5 fold cv score = **`r cv_model1`**.


**Best Model (Model 1) Misclassification Rate**
```{r warning=FALSE}
mean(ifelse(predict(model1,wisc_test) > 0 , "M", "B") != wisc_test$class)
```

The best model (i.e. **`Model 1`**) Test Misclassification rate is **`r mean(ifelse(predict(model1,wisc_test) > 0 , "M", "B") != wisc_test$class)`**

**(b)** In this situation, simply minimizing misclassifications might be a bad goal since false positives and false negatives carry very different consequences. Consider the `M` class as the "positive" label. Consider each of the probabilities stored in `cutoffs` in the creation of a classifier using the **additive** model fit in **(a)**.

```{r}
cutoffs = seq(0.01, 0.99, by = 0.01)
```

That is, consider each of the values stored in `cutoffs` as $c$. Obtain the sensitivity and specificity in the test set for each of these classifiers. Using a single graphic, plot both sensitivity and specificity as a function of the cutoff used to create the classifier. Based on this plot, which cutoff would you use? (0 and 1 have not been considered for coding simplicity. If you like, you can instead consider these two values.)


**Function for Confusion Matrix, Sensitivity, Specificity**
```{r}
#Confustion matrix creation
make_conf_mat = function(predicted, actual){
  table(predicted = predicted, actual = actual)
}

#Sensitivity Function
get_sens = function(conf_mat){
  conf_mat[2, 2] / sum(conf_mat[, 2])
}

#Specificity Function
get_spec = function(conf_mat){
  conf_mat[1, 1] / sum(conf_mat[, 1])
}
```

$$
\hat{C}(\bf x) = 
\begin{cases} 
      1 & \hat{p}({\bf x}) > c \\
      0 & \hat{p}({\bf x}) \leq c 
\end{cases}
$$

**Additive Model**
```{r}
model_add = glm(class ~ ., data = wisc_train, family = binomial)
```

```{r}
sens_99 = seq(1,length(cutoffs))
spec_99 = seq(1,length(cutoffs))
for(i in (1:length(cutoffs))){
  test_predict = ifelse(predict(model_add, wisc_test, type = "response") > cutoffs[i] , "M", "B")
  conf_mat_predict = make_conf_mat(predicted = test_predict, actual = wisc_test$class)
  sens_99[i] = get_sens(conf_mat_predict)
  spec_99[i] = get_spec(conf_mat_predict)
}
```

```{r}
head(sens_99, 10)
head(spec_99, 10)
```

Plot all Specificity and all Sensitivity for all Thresholds Value

```{r}
plot(sens_99~cutoffs,type = "l",col = "darkorange", lwd = 2, lty = 1, xlab = "Threshold - Cutoffs", ylab = "Sensitivity / Specificity")
lines(spec_99~cutoffs,type = "l",col = "dodgerblue", lwd = 2, lty = 1)
legend("topright", legend = c("Sensitivity", "Specificity"), col=c("darkorange","dodgerblue"),lty = c(1,1))
```

```{r}
which.min(abs(sens_99 - spec_99))
cutoffs[which.min(abs(sens_99 - spec_99))]
```


Based on the above plot, it seems that the best threshold to clasify the Model is **`r cutoffs[which.min(abs(sens_99 - spec_99))]`**, where the Sensitivity crossed the Specificitiy. This is point where the Sensitivity is minimum and specificity is maximum at its best

