abline (model, col = "blue", lwd = 2)
curve(predict(model2, newdata = data.frame(x)),lwd = 2, col = "dodgerblue", lty = 1, add = TRUE)
model = lm(y ~ x)
model2 = lm(y ~ poly(x,2))
model4 = lm(y ~ poly(x,4) )
model10 = lm(y ~ poly(x,10) )
plot(y~x, pch = 20, cex = 2, col = "darkgrey")
abline (model, col = "blue", lwd = 2)
curve(predict(model2, newdata = data.frame(x)),lwd = 2, col = "dodgerblue", lty = 1, add = TRUE)
curve(predict(model4, newdata = data.frame(x)),lwd = 2, col = "orange", lty = 2, add = TRUE)
curve(predict(model10, newdata = data.frame(x)),lwd = 2, col = "red", lty = 3, add = TRUE)
plot(y~x, pch = 20, cex = 2, col = "darkgrey", ylim = c(-100,400))
abline (model, col = "blue", lwd = 2)
curve(predict(model2, newdata = data.frame(x)),lwd = 2, col = "dodgerblue", lty = 1, add = TRUE)
curve(predict(model4, newdata = data.frame(x)),lwd = 2, col = "orange", lty = 2, add = TRUE)
curve(predict(model10, newdata = data.frame(x)),lwd = 2, col = "red", lty = 3, add = TRUE)
set.seed(1234)
x = seq(0,10)
set.seed(1234)
y = 3 + x + 4 * x ^2 + rnorm(n = 11, mean = 0, sd = 25)
model = lm(y ~ x)
model2 = lm(y ~ poly(x,2))
model4 = lm(y ~ poly(x,4) )
model10 = lm(y ~ poly(x,10) )
plot(y~x, pch = 20, cex = 2, col = "darkgrey", ylim = c(-100,400))
abline (model, col = "blue", lwd = 2)
curve(predict(model2, newdata = data.frame(x)),lwd = 2, col = "dodgerblue", lty = 1, add = TRUE)
curve(predict(model4, newdata = data.frame(x)),lwd = 2, col = "orange", lty = 2, add = TRUE)
curve(predict(model10, newdata = data.frame(x)),lwd = 2, col = "red", lty = 3, add = TRUE)
set.seed(1234)
x = seq(0,10)
set.seed(1234)
y = 3 + x + 4 * x ^2 + rnorm(n = 11, mean = 0, sd = 25)
model = lm(y ~ x)
model2 = lm(y ~ poly(x,2))
model4 = lm(y ~ poly(x,4) )
model8 = lm(y ~ poly(x,8) )
model10 = lm(y ~ poly(x,10) )
plot(y~x, pch = 20, cex = 2, col = "darkgrey", ylim = c(-100,400))
abline (model, col = "blue", lwd = 2)
curve(predict(model2, newdata = data.frame(x)),lwd = 2, col = "dodgerblue", lty = 1, add = TRUE)
curve(predict(model4, newdata = data.frame(x)),lwd = 2, col = "orange", lty = 2, add = TRUE)
curve(predict(model8, newdata = data.frame(x)),lwd = 2, col = "red", lty = 3, add = TRUE)
sqrt(mean((resid(model2))^2))
sqrt(mean((resid(model2))^2))
sqrt(mean((resid(model4))^2))
sqrt(mean((resid(model8))^2))
sqrt(mean((resid(model2))^2))
set.seed(1234)
x = seq(0,10)
set.seed(1234)
y = 3 + x + 4 * x ^2 + rnorm(n = 11, mean = 0, sd = 25)
model = lm(y ~ x)
model2 = lm(y ~ poly(x,2))
model4 = lm(y ~ poly(x,4) )
model8 = lm(y ~ poly(x,8) )
model10 = lm(y ~ poly(x,10) )
sqrt(mean((resid(model2))^2))
sqrt(mean((resid(model4))^2))
sqrt(mean((resid(model8))^2))
set.seed(1234)
x = seq(0,10)
set.seed(1234)
y = 3 + x + 4 * x ^2 + rnorm(n = 11, mean = 0, sd = 20)
model = lm(y ~ x)
model2 = lm(y ~ poly(x,2))
model4 = lm(y ~ poly(x,4) )
model8 = lm(y ~ poly(x,8) )
model10 = lm(y ~ poly(x,10) )
plot(y~x, pch = 20, cex = 2, col = "darkgrey", ylim = c(-100,400))
abline (model, col = "blue", lwd = 2)
curve(predict(model2, newdata = data.frame(x)),lwd = 2, col = "dodgerblue", lty = 1, add = TRUE)
curve(predict(model4, newdata = data.frame(x)),lwd = 2, col = "orange", lty = 2, add = TRUE)
curve(predict(model8, newdata = data.frame(x)),lwd = 2, col = "red", lty = 3, add = TRUE)
sqrt(mean((resid(model2))^2))
sqrt(mean((resid(model4))^2))
sqrt(mean((resid(model8))^2))
make_poly_data = function(sample_size = 11){
x = seq(0,10)
y = 3 + x + 4 * x ^2 + rnorm(n = sample_size, mean = 0, sd = 20)
data.frame(x,y)
}
set.seed(1234)
poly_data = make_poly_data()
model = lm(y ~ x, data = poly_data)
model2 = lm(y ~ poly(x,2))
model4 = lm(y ~ poly(x,4) )
model8 = lm(y ~ poly(x,8) )
model10 = lm(y ~ poly(x,10) )
sqrt(mean((resid(model2))^2))
sqrt(mean((resid(model4))^2))
sqrt(mean((resid(model8))^2))
model2 = lm(y ~ poly(x,2), data = poly_data)
model = lm(y ~ x, data = poly_data)
model2 = lm(y ~ poly(x,2), data = poly_data)
model4 = lm(y ~ poly(x,4), data = poly_data)
model8 = lm(y ~ poly(x,8), data = poly_data)
model10 = lm(y ~ poly(x,10), data = poly_data)
sqrt(mean((resid(model2))^2))
sqrt(mean((resid(model4))^2))
sqrt(mean((resid(model8))^2))
sqrt(mean((resid(model))^2))
sqrt(mean((resid(model2))^2))
sqrt(mean((resid(model4))^2))
sqrt(mean((resid(model8))^2))
make_poly_data = function(sample_size = 11){
x = seq(0,10)
y = 3 + x + 4 * x ^2 + rnorm(n = sample_size, mean = 0, sd = 20)
data.frame(x,y)
}
set.seed(1234)
poly_data = make_poly_data()
model = lm(y ~ x, data = poly_data)
model2 = lm(y ~ poly(x,2), data = poly_data)
model4 = lm(y ~ poly(x,4), data = poly_data)
model8 = lm(y ~ poly(x,8), data = poly_data)
model10 = lm(y ~ poly(x,10), data = poly_data)
plot(y~x, pch = 20, cex = 2, col = "darkgrey", ylim = c(-100,400))
abline (model, col = "blue", lwd = 2)
curve(predict(model2, newdata = data.frame(x)),lwd = 2, col = "dodgerblue", lty = 1, add = TRUE)
curve(predict(model4, newdata = data.frame(x)),lwd = 2, col = "orange", lty = 2, add = TRUE)
curve(predict(model8, newdata = data.frame(x)),lwd = 2, col = "red", lty = 3, add = TRUE)
sqrt(mean((resid(model))^2))
sqrt(mean((resid(model2))^2))
sqrt(mean((resid(model4))^2))
sqrt(mean((resid(model8))^2))
#LOOCV - RMSE
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
sqrt(mean((resid(model))^2))
sqrt(mean((resid(model2))^2))
#sqrt(mean((resid(model4))^2))
sqrt(mean((resid(model8))^2)) #Best Model = 10.4197
make_poly_data = function(sample_size = 11){
x = seq(0,10)
y = 3 + x + 4 * x ^2 + rnorm(n = sample_size, mean = 0, sd = 20)
data.frame(x,y)
}
set.seed(1234)
poly_data = make_poly_data()
model = lm(y ~ x, data = poly_data)
model2 = lm(y ~ poly(x,2), data = poly_data)
model4 = lm(y ~ poly(x,4), data = poly_data)
model8 = lm(y ~ poly(x,8), data = poly_data)
model10 = lm(y ~ poly(x,10), data = poly_data)
plot(y~x, pch = 20, cex = 2, col = "darkgrey", ylim = c(-100,400))
abline (model, col = "blue", lwd = 2)
curve(predict(model2, newdata = data.frame(x)),lwd = 2, col = "dodgerblue", lty = 1, add = TRUE)
curve(predict(model4, newdata = data.frame(x)),lwd = 2, col = "orange", lty = 2, add = TRUE)
curve(predict(model8, newdata = data.frame(x)),lwd = 2, col = "red", lty = 3, add = TRUE)
sqrt(mean((resid(model))^2))
sqrt(mean((resid(model2))^2))
#sqrt(mean((resid(model4))^2))
sqrt(mean((resid(model8))^2)) #Best Model = 10.4197
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
sqrt(mean((resid(model2) / (1 - hatvalues(model2))) ^ 2))
#sqrt(mean((resid(model4) / (1 - hatvalues(model4))) ^ 2))
sqrt(mean((resid(model8) / (1 - hatvalues(model8))) ^ 2))
exclude = 3
model = lm(y ~ x, data = poly_data[-exclude,])
model2 = lm(y ~ poly(x,2), data = poly_data[-exclude,])
model4 = lm(y ~ poly(x,4), data = poly_data[-exclude,])
model8 = lm(y ~ poly(x,8), data = poly_data[-exclude,])
model10 = lm(y ~ poly(x,10), data = poly_data[-exclude,])
plot(y~x, pch = 20, cex = 2, col = "darkgrey", ylim = c(-100,400))
abline (model, col = "blue", lwd = 2)
curve(predict(model2, newdata = data.frame(x)),lwd = 2, col = "dodgerblue", lty = 1, add = TRUE)
curve(predict(model4, newdata = data.frame(x)),lwd = 2, col = "orange", lty = 2, add = TRUE)
curve(predict(model8, newdata = data.frame(x)),lwd = 2, col = "red", lty = 3, add = TRUE)
exclude = 2
model = lm(y ~ x, data = poly_data[-exclude,])
model2 = lm(y ~ poly(x,2), data = poly_data[-exclude,])
model4 = lm(y ~ poly(x,4), data = poly_data[-exclude,])
model8 = lm(y ~ poly(x,8), data = poly_data[-exclude,])
model10 = lm(y ~ poly(x,10), data = poly_data[-exclude,])
plot(y~x, pch = 20, cex = 2, col = "darkgrey", ylim = c(-100,400))
abline (model, col = "blue", lwd = 2)
curve(predict(model2, newdata = data.frame(x)),lwd = 2, col = "dodgerblue", lty = 1, add = TRUE)
curve(predict(model4, newdata = data.frame(x)),lwd = 2, col = "orange", lty = 2, add = TRUE)
curve(predict(model8, newdata = data.frame(x)),lwd = 2, col = "red", lty = 3, add = TRUE)
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
sqrt(mean((resid(model2) / (1 - hatvalues(model2))) ^ 2))
#sqrt(mean((resid(model4) / (1 - hatvalues(model4))) ^ 2))
sqrt(mean((resid(model8) / (1 - hatvalues(model8))) ^ 2))
ToothGrowth
str(ToothGrowth)
table(supp)
table(ToothGrowth$supp)
?ToothGrowth
table(ToothGrowth+dose)
ToothGrowth$dose = as.numeric(ToothGrowth$dose)
table(ToothGrowth$dose)
lm (len ~ dose + supp, data = ToothGrowth)
table(ToothGrowth$dose)
str(ToothGrowth)
ToothGrowth$dose = as.numeric(ToothGrowth$dose)
str(ToothGrowth)
lm(Fertility~Education*Catholic*Infant.Mortality,data=swiss)
lm(Fertility~Education:Catholic:Infant.Mortality,data=swiss)
lm(Fertility~(.) ^ 2 ,data=swiss)
lm(Fertility~(Education+Catholic+Infant.Mortality) ^ 2 ,data=swiss)
lm(Fertility~Education*Catholic*Infant.Mortality,data=swiss)
lm(Fertility~(Education+Catholic+Infant.Mortality) ^ 3 ,data=swiss)
?car::vif
knitr::opts_chunk$set(cache = TRUE, autodep = TRUE, fig.align = "center")
message("Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred")
sim_logistic_data = function(sample_size = 25, beta_0 = -2, beta_1 = 3) {
x = rnorm(n = sample_size)
eta = beta_0 + beta_1 * x
p = 1 / (1 + exp(-eta))
y = rbinom(n = sample_size, size = 1, prob = p)
data.frame(y, x)
}
set.seed(1)
example_data = sim_logistic_data()
head(example_data)
# ordinary linear regression
fit_lm  = lm(y ~ x, data = example_data)
# logistic regression
fit_glm = glm(y ~ x, data = example_data, family = binomial)
# more detailed call to glm for logistic regression
fit_glm = glm(y ~ x, data = example_data, family = binomial(link = "logit"))
plot(y ~ x, data = example_data,
pch = 20, ylab = "Estimated Probability",
main = "Ordinary vs Logistic Regression")
grid()
abline(fit_lm, col = "darkorange")
curve(predict(fit_glm, data.frame(x), type = "response"),
add = TRUE, col = "dodgerblue", lty = 2)
legend("topleft", c("Ordinary", "Logistic", "Data"), lty = c(1, 2, 0),
pch = c(NA, NA, 20), lwd = 2, col = c("darkorange", "dodgerblue", "black"))
round(coef(fit_glm), 1)
set.seed(1)
example_data = sim_logistic_data(sample_size = 50, beta_0 = 1, beta_1 = -4)
fit_glm = glm(y ~ x, data = example_data, family = binomial)
plot(y ~ x, data = example_data,
pch = 20, ylab = "Estimated Probability",
main = "Logistic Regression, Decreasing Probability")
grid()
curve(predict(fit_glm, data.frame(x), type = "response"),
add = TRUE, col = "dodgerblue", lty = 2)
legend("bottomleft", c("Estimated Probability", "Data"), lty = c(2, 0),
pch = c(NA, 20), lwd = 2, col = c("dodgerblue", "black"))
sim_quadratic_logistic_data = function(sample_size = 25) {
x = rnorm(n = sample_size)
eta = -1.5 + 0.5 * x + x ^ 2
p = 1 / (1 + exp(-eta))
y = rbinom(n = sample_size, size = 1, prob = p)
data.frame(y, x)
}
set.seed(42)
example_data = sim_quadratic_logistic_data(sample_size = 50)
fit_glm = glm(y ~ x + I(x^2), data = example_data, family = binomial)
plot(y ~ x, data = example_data,
pch = 20, ylab = "Estimated Probability",
main = "Logistic Regression, Quadratic Relationship")
grid()
curve(predict(fit_glm, data.frame(x), type = "response"),
add = TRUE, col = "dodgerblue", lty = 2)
legend("left", c("Prob", "Data"), lty = c(2, 0),
pch = c(NA, 20), lwd = 2, col = c("dodgerblue", "black"))
# install.packages("ElemStatLearn")
library(ElemStatLearn)
predict(chd_mod_selected, new_obs, se.fit = TRUE, type = "link")
new_obs = data.frame(
sbp = 148.0,
tobacco = 5,
ldl = 12,
adiposity = 31.23,
famhist = "Present",
typea = 47,
obesity = 28.50,
alcohol = 23.89,
age = 60
)
chd_mod_additive = glm(chd ~ ., data = SAheart, family = binomial)
library(ElemStatLearn)
data("SAheart")
install.packages("ElemStatLearn")
library(ElemStatLearn)
data("SAheart")
chd_mod_additive = glm(chd ~ ., data = SAheart, family = binomial)
anova(chd_mod_ldl, chd_mod_additive, test = "LRT")
chd_mod_selected = step(chd_mod_additive, trace = 0)
anova(chd_mod_selected, chd_mod_additive, test = "LRT")
confint(chd_mod_selected, level = 0.99)
new_obs = data.frame(
sbp = 148.0,
tobacco = 5,
ldl = 12,
adiposity = 31.23,
famhist = "Present",
typea = 47,
obesity = 28.50,
alcohol = 23.89,
age = 60
)
predict(chd_mod_selected, new_obs, se.fit = TRUE, type = "link")
predict(chd_mod_selected, new_obs, se.fit = TRUE, type = "response")
eta_hat$fit + c(-1, 1) * z_crit * eta_hat$se.fit
eta_hat = predict(chd_mod_selected, new_obs, se.fit = TRUE, type = "link")
eta_hat
z_crit = round(qnorm(0.975), 2)
round(z_crit, 2)
eta_hat$fit + c(-1, 1) * z_crit * eta_hat$se.fit
boot::inv.logit(eta_hat$fit + c(-1, 1) * z_crit * eta_hat$se.fit)
predict(chd_mod_selected, new_obs, se.fit = TRUE, type = "link")
predict(chd_mod_selected, new_obs, se.fit = TRUE, type = "link", level = "confidence")
predict(chd_mod_selected, new_obs, se.fit = TRUE, type = "link", level = "confidence", interval = 0.95)
setwd("~/Documents/GitHub/Illinois/STAT 420 -  Statistical Modeling in R/HW10/w10-hw")
model = glm(survived ~ pclass + sex + age + sex:age, data = ptitanic_trn, family = binomial)
ptitanic = na.omit(ptitanic)
set.seed(42)
trn_idx = sample(nrow(ptitanic), 300)
ptitanic_trn = ptitanic[trn_idx, ]
ptitanic_tst = ptitanic[-trn_idx, ]
# install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
data("ptitanic")
ptitanic = na.omit(ptitanic)
set.seed(42)
trn_idx = sample(nrow(ptitanic), 300)
ptitanic_trn = ptitanic[trn_idx, ]
ptitanic_tst = ptitanic[-trn_idx, ]
model = glm(survived ~ pclass + sex + age + sex:age, data = ptitanic_trn, family = binomial)
test_predict = ifelse(predict(model,ptitanic_tst) > 0, "survived", "died")
test_predict
conf_mat = function(predict, actual){
table(predict,actual)
}
conf_mat(test_predict,ptitanic_tst$survived)
str(ptitanic_tst)
conf_mat(test_predict,ptitanic_tst$survived)
conf_mat = conf_mat(test_predict,ptitanic_tst$survived)
sen = conf_mat[2,2] / conf_mat[,2]
spe = conf[1,1] / conf_mat [,1]
spe = conf_mat[1,1] / conf_mat [,1]
sen
spe
conf_mat[2,2]
sen = conf_mat[2,2] / sum(conf_mat[,2])
sen
spe = conf_mat[1,1] / sum(conf_mat [,1])
spe
conf_mat(test_predict,ptitanic_tst$survived)
conf_mat = function(predict, actual){
table(predict,actual)
}
conf_mat(test_predict,ptitanic_tst$survived)
conf_mat = function(predict, actual){
#table(predict,actual)
table(actual,predict)
}
conf_mat(test_predict,ptitanic_tst$survived)
beta_0 = 2
beta_1 = -1
beta_2 = -1
x_1 = 1
x_2 = 0
eta = beta_0 + beta_1 * x_1  + beta_2 * x_2
p = (1 / (1 + exp(-eta)))
p
?glm
?cv.glm
mtcars
str(mtcars)
model = glm(am ~ mpg + hp + qsec, data = mtcars, family = binomial)
summary(model)
summary(model)$coefficient
summary(model)$coefficient[4,1]
beta_0 = -3
beta_1 = 1
beta_2 = 2
beta_3 = 3
x1 = -1
x2 = 0.5
x3 = 0.25
eta = beta_0 + beta_1 * x1 + beta_2 * x2 + beta_3 * x3
p = 1/(1+exp(-eta))
1- p
summary(model)
summary(model)$coefficient
summary(model)$coefficient[2,1]
new_data = data.frame(mpg = 19, hp = 150, qsec = 19)
predict(model, newdata = new_data, type = "response")
summary(model)$coefficient[1,1] + summary(model)$coefficient[2,1]*19 +
summary(model)$coefficient[3,1]*150 + summary(model)$coefficient[4,1]*19
new_data = data.frame(mpg = 22, hp = 123, qsec = 18)
predict(model, newdata = new_data, type = "response")
#6@
model = glm(am ~ mpg + hp + qsec, data = mtcars, family = binomial)
model2 = glm(am ~ 1, data = mtcars, family = binomial)
anova(model, model2, test = "LRT")
summary(model)
summary(model)$coefficient
summary(model)$coefficient[3,4]
library(MASS)
Pima_tr = MASS::Pima.tr
Pima_te = MASS::Pima.te
str(Pima_tr)
model = glm(type ~ glu + ped + I(glu^2) + I(ped^2) + glu:ped, family = binomial)
library(MASS)
Pima_tr = MASS::Pima.tr
Pima_te = MASS::Pima.te
model = glm(type ~ glu + ped + I(glu^2) + I(ped^2) + glu:ped, family = binomial, data = Pima_tr)
summary(model)$coefficient
summary(model)$coefficient[5,1]
#9@-0.3595626
predict(model,newdata = Pima_te, type = "response")
#9@-0.3595626
mean (predict(model,newdata = Pima_te, type = "response") > 0.80)
model_aic = step(model , direction = "backward")
model_aic = step(model , direction = "backward", trace= 0)
summary(model_aic)
summary(model_aic)$coefficient
#11@
model = glm(type ~ (.) ^ 2, family = binomial, data = Pima_tr)
summary(model)
summary(model)$coefficient
model_aic = step(model, direction = "backward", trace = 0)
summary(model_aic)
table(Pima_tr$type)
mean(ifelse(predict(model) > 0, 'Yes', 'No') != Pima_te$type)
#14@
model = glm(type ~ ., data = Pima_tr)
#14@
model = glm(type ~ ., data = Pima_tr)
#14@
model = glm(type ~ ., family = "binomial", data = Pima_tr)
mean(ifelse(predict(model) > 0, 'Yes', 'No') != Pima_te$type)
mean(ifelse(predict(model,Pima_te) > 0, 'Yes', 'No') != Pima_te$type)
model = glm(type ~ ., family = "binomial", data = Pima_tr)
mean(ifelse(predict(model,Pima_te) > 0, 'Yes', 'No') != Pima_te$type)
mean(ifelse(predict(model,Pima_te,type = "response") > 0, 'Yes', 'No') != Pima_te$type)
conf_mat = conf_mat(predict, Pima_te$type)
predict
ifelse(predict(model,Pima_te) > 0, 'Yes', 'No')
predict_pi = ifelse(predict(model,Pima_te) > 0, 'Yes', 'No')
conf_mat = conf_mat(predict_pi, Pima_te$type)
conf_mat
conf_mat[2,2]/conf_mat[,2]
conf_mat[2,2]/sum(conf_mat[,2])
predict_pi = ifelse(predict(model,Pima_te,type = "response") > 0.3, 'Yes', 'No')
conf_mat = conf_mat(predict_pi, Pima_te$type)
conf_mat[2,2]/sum(conf_mat[,2])
conf_mat = function(predict,actual){
table(predict, actual)
}
predict_pi = ifelse(predict(model,Pima_te,type = "response") > 0.3, 'Yes', 'No')
conf_mat = conf_mat(predict_pi, Pima_te$type)
conf_mat[2,2]/sum(conf_mat[,2])
#14@0.741573
model = glm(type ~ ., family = "binomial", data = Pima_tr)
conf_mat = function(predict,actual){
table(predict, actual)
}
conf_mat = function(predict,actual){
table(predict, actual)
}
model = glm(type ~ ., family = "binomial", data = Pima_tr)
conf_mat = function(predict,actual){
table(predict, actual)
}
predict_pi = ifelse(predict(model,Pima_te) > 0, 'Yes', 'No')
conf_mat = conf_mat(predict_pi, Pima_te$type)
conf_mat[2,2]/sum(conf_mat[,2])
#10@3
model = glm(type ~ ., family = binomial, data = Pima_tr)
model_aic = step(model , direction = "backward", trace= 0)
summary(model_aic)$coefficient
library(MASS)
Pima_tr = MASS::Pima.tr
Pima_te = MASS::Pima.te
model = glm(type ~ glu + ped + I(glu^2) + I(ped^2) + glu:ped, family = binomial, data = Pima_tr)
summary(model)$coefficient[5,1]
#9@0.07228916
mean (predict(model,newdata = Pima_te, type = "response") > 0.80)
#6@-35.75
model = glm(am ~ mpg + hp + qsec, data = mtcars, family = binomial)
model2 = glm(am ~ 1, data = mtcars, family = binomial)
anova(model, model2, test = "LRT")
fit_null = glm(am ~ 1, data = mtcars, family = "binomial")
fit_full = glm(am ~ mpg + hp + qsec, data = mtcars, family = "binomial")
anova(fit_null, fit_full, test = "LRT")[2, "Deviance"]
anova(fit_null, fit_full, test = "LRT")
#6@-35.75
model = glm(am ~ mpg + hp + qsec, data = mtcars, family = binomial)
model2 = glm(am ~ 1, data = mtcars, family = binomial)
anova(model2, model, test = "LRT")
anova(model, model2, test = "LRT")
#6@-35.75
model2 = glm(am ~ 1, data = mtcars, family = binomial)
model = glm(am ~ mpg + hp + qsec, data = mtcars, family = binomial)
anova(model2, model, test = "LRT")
#2@-4.040952
model = glm(am ~ mpg + hp + qsec, data = mtcars, family = binomial)
summary(model)$coefficient[4,1]
#3@2.29643
summary(model)$coefficient[2,1]
#4@-8.338686
new_data = data.frame(mpg = 19, hp = 150, qsec = 19)
predict(model,newdata = new_data)
extractAIC(model)
(sqrt(sum(((resid(model)/(1-hatvalues(model))) ^ 2)))
(sqrt(sum(((resid(model)/(1-hatvalues(model))) ^ 2))))
?step
diamond_mod_log_log = lm(log(price) ~ log(carat), data = diamonds)
mtcars
str(mtcars)
lm(mpg ~ hp * disp)
lm(mpg ~ hp * disp, data = mtcars)
lm(mpg ~ (hp + disp) ^ 2, data = mtcars)
