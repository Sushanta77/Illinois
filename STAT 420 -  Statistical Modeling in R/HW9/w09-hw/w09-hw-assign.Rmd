---
title: "Week 9 - Homework"
author: "STAT 420, Summer 2019, Sushanta Panda"
date: ''
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
editor_options: 
  chunk_output_type: inline
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
```

## Exercise 1 (`longley` Macroeconomic Data)

The built-in dataset `longley` contains macroeconomic data for predicting employment. We will attempt to model the `Employed` variable.

```{r, eval = FALSE}
View(longley)
?longley
```

**(a)** What is the largest correlation between any pair of predictors in the dataset?

```{r}
cor(longley)
max(cor(longley)[cor(longley) != 1])
which(cor(longley) == max(cor(longley)[cor(longley) != 1]), arr.ind = TRUE)
```

The largest correlation between any pair of predictors in the dataset is **`r max(cor(longley)[cor(longley) != 1])`** whcich is between `Year` and `GNP`

**(b)** Fit a model with `Employed` as the response and the remaining variables as predictors. Calculate and report the variance inflation factor (VIF) for each of the predictors. Which variable has the largest VIF? Do any of the VIFs suggest multicollinearity?

**Fit the model eith `Employed` as the response and all remaining variables as predictors**
```{r}
fit_lin = lm(Employed ~ . , data = longley)
fit_lin
```

**Derive the Varience Inflation Factor (VIF) for each variable**
```{r}
car::vif(fit_lin)
```

**Derive the maximim Varience Inflation Factor (VIF) for the variable**
```{r}
which.max(car::vif(fit_lin))
```
The variable `r names(which.max(car::vif(fit_lin)))` **`r names(which.max(car::vif(fit_lin)))`** has the largest Varience Inflation Factor (VIF) and value is: **`r max(car::vif(fit_lin))`**

From the VIF it seems that the varibale `GNP.deflator` (** > 130**), `GNP` (** > 1500**), `Population`(** > 300**) and `Year`(** > 750**) has the higher VIF , hence **there is multicolinearity exists between the variables**

**(c)** What proportion of the observed variation in `Population` is explained by a linear relationship with the other predictors?
```{r}
fit_lin_population = lm(Population ~ .-Employed, data = longley)
summary(fit_lin_population)$r.squared
```

The Proportion of the observerd variation in `Population` is explained by a linear relationhip with other predictors is: **`r summary(fit_lin_population)$r.squared`**

**(d)** Calculate the partial correlation coefficient for `Population` and `Employed` **with the effects of the other predictors removed**.

```{r}
fit_lin_Employed = lm(Employed ~ 1, data = longley)
fit_lin_Population = lm(Population ~ 1, data = longley)
cor(resid(fit_lin_Employed), resid(fit_lin_Population))
```

The partial correlation coefficient for `population` and `Employed` **with the effects of the other predictors removed** is : **`r cor(resid(fit_lin_Employed), resid(fit_lin_Population))`**

**(e)** Fit a new model with `Employed` as the response and the predictors from the model in **(b)** that were significant. (Use $\alpha = 0.05$.) Calculate and report the variance inflation factor for each of the predictors. Which variable has the largest VIF? Do any of the VIFs suggest multicollinearity?

**Identifying predictor from the model in (b) which are significanct**
```{r}
summary(fit_lin)$coefficient[,'Pr(>|t|)'][summary(fit_lin)$coefficient[,'Pr(>|t|)'] < 0.05]
```

The predictors which are significance from the model **(b)** are **`Unemployed`**,**`Armed.Forces`** and **`Year`**

**Fit a new model with these predictors which are significant**
```{r}
fit_model_sign = lm(Employed ~ Unemployed + Armed.Forces + Year, data = longley)
fit_model_sign
```

```{r}
car::vif(fit_model_sign)
```

The variation inflation factor of `Unemployed` is: **`r car::vif(fit_model_sign)[1]`**, `Armed.Forces` is: **`r car::vif(fit_model_sign)[2]`**, `Year` is: **`r car::vif(fit_model_sign)[3]`**


```{r}
which.max(car::vif(fit_model_sign))
```

The variable which has highest variation inflation factor(VIF) is: **`r names(which.max(car::vif(fit_model_sign)))`** having VIF is: **`r max(car::vif(fit_model_sign))`**

**(f)** Use an $F$-test to compare the models in parts **(b)** and **(e)**. Report the following:

- The null hypothesis

$$
H_0 : \beta_{GNP.deflator} = \beta_{GNP} = \beta_{Population} = 0
$$


- The test statistic

    ```{r}
    null_model = lm(Employed ~ Unemployed + Armed.Forces + Year, data = longley)
    full_model = lm(Employed ~ . , data = longley)  
    anova(null_model,full_model)
    ```


- The distribution of the test statistic under the null hypothesis

The distribution of the test statiscs under the null hypothesis for the following parameter is as below

**Distribution of the F value **

The ditribution of the **F value** is **F distribution with Degree of Freedom between `3` (df1) and `12` (df2)**
$$
F \sim F(p - 1, n - p), \text {where n = 16 and p = 4}
$$
The ditribution of the **P value** is **Uniform distribution**
$$
\text{p-value} \sim \text{Unif}(0, 1).
$$
    
The ditribution of the **R2 value** is **Beta distribution** with parameter of shape1 = 4/2, asd shape2 = 11/2
    
$$
R ^ 2 \sim \text{Beta}\left(\frac{p}{2}, \frac{n - p - 1}{2}\right) , \text {where n = 16 and p = 4}
$$

- The p-value

    ```{r}
    anova(null_model,full_model)$'Pr(>F)'[2]
    ```

    The P value is **`r anova(null_model,full_model)$'Pr(>F)'[2]`**


- A decision

    Since the P value from the anova P test is **`r anova(null_model,full_model)$'Pr(>F)'[2]`** which is greater than for any $\alpha$ (0.1, 0.05), hence we **failed to reject the null hypothesis**, means we prefer the **NULL Model**


- Which model you prefer, **(b)** or **(e)**

    Since we failed to reject the NULL model, we prefer the NULL model, i.e. model comes from the **(e)**

**(g)** Check the assumptions of the model chosen in part **(f)**. Do any assumptions appear to be violated?

```{r, echo = FALSE}
plot_fitted_resid = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  plot(fitted(model), resid(model), 
       col = pointcol, pch = 20, cex = 1.5,
       xlab = "Fitted", ylab = "Residuals")
  abline(h = 0, col = linecol, lwd = 2)
}

plot_qq = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  qqnorm(resid(model), col = pointcol, pch = 20, cex = 1.5)
  qqline(resid(model), col = linecol, lwd = 2)
}
```

```{r}
plot_fitted_resid(model = null_model)
```

The **constant varience** assumption seems to be **suspect**, as from the above plot, it seem that the residuals are not uniformly ditributed accross the dark orange like. Hence it violates the constant varience assumption 


```{r}
plot_qq(model = null_model)
```

The **normality** assumptions seems to be **suspect**, as from the above Q-Q plot, it seems that it has a heavy tail on both higher and lower quantiles. Hence it also violates the normality assumptions


***

## Exercise 2 (`Credit` Data)

For this exercise, use the `Credit` data from the `ISLR` package. Use the following code to remove the `ID` variable which is not useful for modeling.

```{r}
library(ISLR)
data(Credit)
Credit = subset(Credit, select = -c(ID))
```

Use `?Credit` to learn about this dataset.

**(a)** Find a "good" model for `balance` using the available predictors. Use any methods seen in class except transformations of the response. The model should:

- Reach a LOOCV-RMSE below `135`
- Obtain an adjusted $R^2$ above `0.90`
- Fail to reject the Breusch-Pagan test with an $\alpha$ of $0.01$
- Use fewer than 10 $\beta$ parameters

Store your model in a variable called `mod_a`. Run the two given chunks to verify your model meets the requested criteria. If you cannot find a model that meets all criteria, partial credit will be given for meeting at least some of the criteria.

```{r}
mod_a = lm(Balance ~ log(Income) + Limit + Cards + Age + Gender + 
                   Student, data = Credit)
```


```{r, message = FALSE, warning = FALSE}
library(lmtest)

get_bp_decision = function(model, alpha) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_num_params = function(model) {
  length(coef(model))
}

get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}
```

```{r, eval = TRUE}
get_loocv_rmse(mod_a)
get_adj_r2(mod_a)
get_bp_decision(mod_a, alpha = 0.01)
get_num_params(mod_a)
```

```{r, eval = TRUE}
get_loocv_rmse(mod_a) < 135
get_adj_r2(mod_a) > 0.90
get_bp_decision(mod_a, alpha = 0.01) == "Fail to Reject"
get_num_params(mod_a) < 10
```


**(b)** Find another "good" model for `balance` using the available predictors. Use any methods seen in class except transformations of the response. The model should:

- Reach a LOOCV-RMSE below `125`
- Obtain an adjusted $R^2$ above `0.91`
- Fail to reject the Shapiro-Wilk test with an $\alpha$ of $0.01$
- Use fewer than 25 $\beta$ parameters

Store your model in a variable called `mod_b`. Run the two given chunks to verify your model meets the requested criteria. If you cannot find a model that meets all criteria, partial credit will be given for meeting at least some of the criteria.

```{r}
mod_b = lm(Balance ~ log(Income) + Limit + Cards + Age + Gender + Student +
                           I(log(Income)^2) + I(Limit^2), data = Credit)
```


```{r, message = FALSE, warning = FALSE}
library(lmtest)

get_bp_decision = function(model, alpha) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_num_params = function(model) {
  length(coef(model))
}

get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}
```

```{r, eval = TRUE}
get_loocv_rmse(mod_b)
get_adj_r2(mod_b)
get_sw_decision(mod_b, alpha = 0.01)
get_num_params(mod_b)
```

```{r, eval = TRUE}
get_loocv_rmse(mod_b) < 125
get_adj_r2(mod_b) > 0.90 
get_sw_decision(mod_b, alpha = 0.01) == "Fail to Reject"
get_num_params(mod_b) < 25
```


***

## Exercise 3 (`Sacramento` Housing Data)

For this exercise, use the `Sacramento` data from the `caret` package. Use the following code to perform some preprocessing of the data.

```{r warning=FALSE}
library(lattice)
library(caret)
library(ggplot2)

data(Sacramento)
sac_data = Sacramento
sac_data$limits = factor(ifelse(sac_data$city == "SACRAMENTO", "in", "out"))
sac_data = subset(sac_data, select = -c(city, zip))
```

Instead of using the `city` or `zip` variables that exist in the dataset, we will simply create a variable (`limits`) indicating whether or not a house is technically within the city limits of Sacramento. (We do this because they would both be factor variables with a **large** number of levels. This is a choice that is made due to laziness, not necessarily because it is justified. Think about what issues these variables might cause.)

Use `?Sacramento` to learn more about this dataset.

A plot of longitude versus latitude gives us a sense of where the city limits are.

```{r}
qplot(y = longitude, x = latitude, data = sac_data,
      col = limits, main = "Sacramento City Limits ")
```

After these modifications, we test-train split the data.

```{r}
set.seed(420)
sac_trn_idx  = sample(nrow(sac_data), size = trunc(0.80 * nrow(sac_data)))
sac_trn_data = sac_data[sac_trn_idx, ]
sac_tst_data = sac_data[-sac_trn_idx, ]
```

The training data should be used for all model fitting. Our goal is to find a model that is useful for predicting home prices.

**(a)** Find a "good" model for `price`. Use any methods seen in class. The model should reach a LOOCV-RMSE below 77,500 in the training data. Do not use any transformations of the response variable.

```{r}
good_model = lm(price ~ beds + baths + sqft + type + latitude + 
    longitude + limits + beds:sqft + beds:longitude + baths:limits + 
    sqft:longitude + sqft:limits + type:latitude + latitude:longitude + 
    longitude:limits, data = sac_trn_data)
good_model
sqrt(mean((resid(good_model) / (1 - hatvalues(good_model)))^2))
```

The LOOCV-RMSE is **`r sqrt(mean((resid(good_model) / (1 - hatvalues(good_model)))^2))`**


```{r}
sqrt(mean((resid(good_model) / (1 - hatvalues(good_model)))^2)) < 77500
```


**(b)** Is a model that achieves a LOOCV-RMSE below 77,500 useful in this case? That is, is an average error of 77,500 low enough when predicting home prices? To further investigate, use the held-out test data and your model from part **(a)** to do two things:

- Calculate the average percent error:
\[
\frac{1}{n}\sum_i\frac{|\text{predicted}_i - \text{actual}_i|}{\text{predicted}_i} \times 100
\]

```{r}
predict_price = predict(good_model, newdata = sac_tst_data[,c("beds","baths","sqft","type","latitude","longitude","limits")])

average_percent_error = (mean(abs(predict_price - sac_tst_data$price) / predict_price)) * 100
average_percent_error
```

The average percent error is around **`r average_percent_error`**

- Plot the predicted versus the actual values and add the line $y = x$.

```{r}
xplot = seq(0,10,by=0.1)
plot(predict_price~sac_tst_data$price, col = "dodgerblue", xlim=c(0,700000), ylim=c(0,700000),pch = 1, cex = 1, xlab = "Actual Price", ylab = "Predicted Price", main = "Actual Price Versus Predicted Price")
abline(a = 0, b = 1, col = "darkorange", lwd = 2)
```

Based on all of this information, argue whether or not this model is useful.

The model seems to be usefull for lower price value, howver not that great, but is not at all usefull for higher price value. The reason because of the fact that, the Total Percent of error is around 23% and from the above plots it also sees that, the predicted price is not close to the orange line (where the predicted value should match with the actual value) for the lower value. As the price goes high, the difference between the predicted price versus actual price seems wider.

***

## Exercise 4 (Does It Work?)

In this exercise, we will investigate how well backwards AIC and BIC actually perform. For either to be "working" correctly, they should result in a low number of both **false positives** and **false negatives**. In model selection,

- **False Positive**, FP: Incorrectly including a variable in the model. Including a *non-significant* variable
- **False Negative**, FN: Incorrectly excluding a variable in the model. Excluding a *significant* variable

Consider the **true** model

\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \beta_5 x_5 + \beta_6 x_6 + \beta_7 x_7 + \beta_8 x_8 + \beta_9 x_9 + \beta_{10} x_{10} + \epsilon
\]

where $\epsilon \sim N(0, \sigma^2 = 4)$. The true values of the $\beta$ parameters are given in the `R` code below.

```{r}
beta_0  = 1
beta_1  = -1
beta_2  = 2
beta_3  = -2
beta_4  = 1
beta_5  = 1
beta_6  = 0
beta_7  = 0
beta_8  = 0
beta_9  = 0
beta_10 = 0
sigma = 2
```

Then, as we have specified them, some variables are significant, and some are not. We store their names in `R` variables for use later.

```{r}
not_sig  = c("x_6", "x_7", "x_8", "x_9", "x_10")
signif = c("x_1", "x_2", "x_3", "x_4", "x_5")
```

We now simulate values for these `x` variables, which we will use throughout part **(a)**.

```{r}
set.seed(420)
n = 100
x_1  = runif(n, 0, 10)
x_2  = runif(n, 0, 10)
x_3  = runif(n, 0, 10)
x_4  = runif(n, 0, 10)
x_5  = runif(n, 0, 10)
x_6  = runif(n, 0, 10)
x_7  = runif(n, 0, 10)
x_8  = runif(n, 0, 10)
x_9  = runif(n, 0, 10)
x_10 = runif(n, 0, 10)
```

We then combine these into a data frame and simulate `y` according to the true model.

```{r}
sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
  y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + 
      beta_5 * x_5 + rnorm(n, 0 , sigma)
)
```

We do a quick check to make sure everything looks correct.

```{r}
head(sim_data_1)
```

Now, we fit an incorrect model.

```{r}
fit = lm(y ~ x_1 + x_2 + x_6 + x_7, data = sim_data_1)
coef(fit)
```

Notice, we have coefficients for `x_1`, `x_2`, `x_6`, and `x_7`. This means that `x_6` and `x_7` are false positives, while `x_3`, `x_4`, and `x_5` are false negatives.

To detect the false negatives, use:

```{r}
# which are false negatives?
!(signif %in% names(coef(fit)))
```

To detect the false positives, use:

```{r}
# which are false positives?
names(coef(fit)) %in% not_sig
```

Note that in both cases, you could `sum()` the result to obtain the number of false negatives or positives.

**(a)** Set a seed equal to your birthday; then, using the given data for each `x` variable above in `sim_data_1`, simulate the response variable `y` 300 times. Each time,

- Fit an additive model using each of the `x` variables.
- Perform variable selection using backwards AIC.
- Perform variable selection using backwards BIC.
- Calculate and store the number of false negatives for the models chosen by AIC and BIC.
- Calculate and store the number of false positives for the models chosen by AIC and BIC.

Calculate the rate of false positives and negatives for both AIC and BIC. Compare the rates between the two methods. Arrange your results in a well formatted table.

######Run the Simulations, fitting the model, calculate the AIC and BIC
```{r message=FALSE, warning=TRUE}
library(knitr)
birthday = 19770411
set.seed(birthday)
num_sim = 300
mod1_false_negative_aic = 0 
mod1_false_postive_aic = 0
mod1_false_negative_bic = 0
mod1_false_postive_bic = 0

for(i in 1:num_sim){
  sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
  y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + 
      beta_5 * x_5 + rnorm(n, 0 , sigma)
  )  
  
  mod1_fit_add = lm(y ~ ., data = sim_data_1)
  
  mod1_fit_add_aic_back = step(mod1_fit_add, direction = "backward",trace=0)
  mod1_false_negative_aic = mod1_false_negative_aic + sum(!(signif %in% names(coef(mod1_fit_add_aic_back)))) # which are false negatives?
  mod1_false_postive_aic = mod1_false_postive_aic + sum(names(coef(mod1_fit_add_aic_back)) %in% not_sig) # which are false positives?
  
  mod1_fit_add_bic_back = step(mod1_fit_add, direction = "backward", k = log(n),trace=0)
  mod1_false_negative_bic = mod1_false_negative_bic + sum(!(signif %in% names(coef(mod1_fit_add_bic_back)))) # which are false negatives?
  mod1_false_postive_bic = mod1_false_postive_bic + sum(names(coef(mod1_fit_add_bic_back)) %in% not_sig) # which are false positives?
}
```

######Derive the Rate of the AIC and BIC for False Negetive, False Positive
```{r}
mod1_rate_false_negative_aic = mod1_false_negative_aic / num_sim
mod1_rate_false_postive_aic = mod1_false_postive_aic / num_sim
  
mod1_rate_false_negative_bic = mod1_false_negative_bic / num_sim
mod1_rate_false_postive_bic = mod1_false_postive_bic / num_sim

mod1_results = data.frame(
  FN = c(mod1_rate_false_negative_aic,mod1_rate_false_negative_bic),
  FP = c(mod1_rate_false_postive_aic,mod1_rate_false_postive_bic)
)
rownames(mod1_results) = c("AIC", "BIC")

mod1_results
```
######Results of the AIC, BIC in the form of a table for Model 1
```{r}
kable(mod1_results, format = "pandoc",padding = 2,caption = "Model 1 - Compare AIC BIC against False Positive (FP), False negative(FN)")
```



**(b)** Set a seed equal to your birthday; then, using the given data for each `x` variable below in `sim_data_2`, simulate the response variable `y` 300 times. Each time,

- Fit an additive model using each of the `x` variables.
- Perform variable selection using backwards AIC.
- Perform variable selection using backwards BIC.
- Calculate and store the number of false negatives for the models chosen by AIC and BIC.
- Calculate and store the number of false positives for the models chosen by AIC and BIC.

Calculate the rate of false positives and negatives for both AIC and BIC. Compare the rates between the two methods. Arrange your results in a well formatted table. Also compare to your answers in part **(a)** and suggest a reason for any differences.

```{r}
set.seed(420)
x_1  = runif(n, 0, 10)
x_2  = runif(n, 0, 10)
x_3  = runif(n, 0, 10)
x_4  = runif(n, 0, 10)
x_5  = runif(n, 0, 10)
x_6  = runif(n, 0, 10)
x_7  = runif(n, 0, 10)
x_8  = x_1 + rnorm(n, 0, 0.1)
x_9  = x_1 + rnorm(n, 0, 0.1)
x_10 = x_2 + rnorm(n, 0, 0.1)

sim_data_2 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
  y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + 
      beta_5 * x_5 + rnorm(n, 0 , sigma)
)
```

######Run the Simulations, fitting the model, calculate the AIC and BIC
```{r}
library(knitr)
birthday = 19770411
set.seed(birthday)
num_sim = 300

mod2_false_negative_aic = 0 
mod2_false_postive_aic = 0
mod2_false_negative_bic = 0
mod2_false_postive_bic = 0


for(i in 1:num_sim){
  sim_data_2 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
  y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + 
      beta_5 * x_5 + rnorm(n, 0 , sigma)
  )

  mod2_fit_add = lm(y ~ ., data = sim_data_2)
  
  mod2_fit_add_aic_back = step(mod2_fit_add, direction = "backward",trace=0)
  mod2_false_negative_aic = mod2_false_negative_aic + sum(!(signif %in% names(coef(mod2_fit_add_aic_back)))) # which are false negatives?
  mod2_false_postive_aic = mod2_false_postive_aic + sum(names(coef(mod2_fit_add_aic_back)) %in% not_sig) # which are false positives?
  
  mod2_fit_add_bic_back = step(mod2_fit_add, direction = "backward", k = log(n),trace=0)
  mod2_false_negative_bic = mod2_false_negative_bic + sum(!(signif %in% names(coef(mod2_fit_add_bic_back)))) # which are false negatives?
  mod2_false_postive_bic = mod2_false_postive_bic + sum(names(coef(mod2_fit_add_bic_back)) %in% not_sig) # which are false positives?
}

```

######Derive the Rate of the AIC and BIC for False Negetive, False Positive
```{r}
mod2_rate_false_negative_aic = mod2_false_negative_aic / num_sim
mod2_rate_false_postive_aic = mod2_false_postive_aic / num_sim
  
mod2_rate_false_negative_bic = mod2_false_negative_bic / num_sim
mod2_rate_false_postive_bic = mod2_false_postive_bic / num_sim

mod2_results = data.frame(
  FN = c(mod2_rate_false_negative_aic,mod2_rate_false_negative_bic),
  FP = c(mod2_rate_false_postive_aic,mod2_rate_false_postive_bic)
)
rownames(mod2_results) = c("AIC", "BIC")
mod2_results
```
######Results of the AIC, BIC in the form of a table for Model 2
```{r}
kable(mod2_results, format = "pandoc",padding = 2,caption = "Model 2 - Compare AIC BIC against False Positive (FP), False negative(FN)")
```

######Creating data frame of the output of the Model 1 and Model 2
```{r}

final_Results = data.frame(
  Model1_FN = c(mod1_rate_false_negative_aic,mod1_rate_false_negative_bic),
  Model2_FN = c(mod2_rate_false_negative_aic,mod2_rate_false_negative_bic),
  Model1_FP = c(mod1_rate_false_postive_aic,mod1_rate_false_postive_bic),  
  Model2_FP = c(mod2_rate_false_postive_aic,mod2_rate_false_postive_bic)
)
rownames(final_Results) = c("AIC","BIC")
final_Results

```
######Results of the AIC, BIC in the form of a table for Model 1 & Model 2
```{r}
kable(final_Results, format = "pandoc",padding = 2,caption = "Model 1 & Model 2 Compare AIC BIC against False Positive (FP), False negative(FN)")
```

In the `sim_data_2`, the variable x_8, x_9 and x_10 has `exact collinearity` with the x_1, x_2 and x_2 respectively, where as this collinearity doesn't exists in `sim_data_1`. Hence False Negetive (FN) in the model 1 (`sim_data_1`) is zero(0), means none of the significance parameters have moved out from the model (via AIC or BIC). On the contrary, it happened in the Model 2 (`sim_data_2`) because AIC/BIC picks x_8 / x_9 / x_10 instead of x_1 or x_2 in the model because of collinearity.

Since the Model 2 (`sim_data_2`) picks sometime the x_8 / x_9  or x_10 instead of x_1 / x_2 (because of collinearity), the False positive have increased in Moddel 2 (`sim_data_2`) as compared to the Model 1 (`sim_data_1`).



