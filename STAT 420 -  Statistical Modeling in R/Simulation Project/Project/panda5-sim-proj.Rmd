---
title: "panda5-sim-proj"
author: "Sushanta Panda"
date: "6/21/2019"
output: 
  html_document: 
    toc: yes
editor_options: 
  chunk_output_type: console
---
## [1] Simulation Study 1: Significance of Regression

###[1.1] Introduction
In this simulation I will investigate the significance of the regression test. I will simulate 2 different model

Below is the **1st model**, which is the **Signifianct model**
\[ Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i.\]

where $\epsilon_i$ follows a Normal distribution of mean = 0 and Varience = $\sigma^2$

We have provided the value of the parameters as

 - β0 = 3
 - β1 = 1
 - β2 = 1
 - β3 = 1


Below is the **2nd model**, which is a **Non-Signifianct model**
\[ Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i.\]

where $\epsilon_i$ follows a Normal distribution of mean = 0 and Varience = $\sigma^2$

We have provided the value of the parameters as

 - β0 = 3
 - β1 = 0
 - β2 = 0
 - β3 = 0
 
 - σ ∈ (1,5,10), **three distinct values of sigma**
 - n = 25
 
I will simulate 2500 simulations for both the models for each value of sigma, so around 2500 X 3 = **7500** for each model, so TOTAL 2 X 7500 = **15,000** times for both the model

I need to show there are around 15000 times the model is being trained as part of the simulations

####[1.1.1] Goal

As part of this exercise, the following are the Goals to achieve

 - Simulate the MLR models (2 models) for 2500 times for a given value of σ
 - Plot the empirical distribution of the for the following 3 parameters 
     - F statistics
     - p-value
     - R square
  - Discuss on the following points in the **Disucssion** section
    - Do we know the true distribution of the following ?
         - F statistics
         - p-value
         - R square
     - How do the empirical distributions compare with the true distribution ?
     - How R2 is related to the sigma

 In order to achieve these goals, we have various methods in the form of R codes, markdown
 
###[1.2] Methods
Below is the code which will runs the 2500 simulations against each sigma (σ)

 - Will Create 2 matrix one for each model (significance and non-significance) which will store the F statistics, p-value and R2 for each sigma value (1, 5 and 10)
 - Once the 2 matrix are populated, will be used to plot the empirical distribution, as well as to compare with the true distribution
 
####[1.2.1] Simulation function for the both signifiance and non-signifiance model

```{r}
birthday = 19770411
set.seed(birthday)

#Variable declaration
n = 25
sigma = c(1,5,10)
no_loop = 2500

#function to generate samples for both significance and non-significance model
mlr_sim = function(n_in,sd=1,signifincance_ind=1){
  exer_data = read.csv("study_1.csv")
  
  epsilon = rnorm(n_in,mean = 0,sd = sd)
  if (signifincance_ind){
    beta_0 = 3
    beta_1 = 1
    beta_2 = 1
    beta_3 = 1 
    exer_data[,1] = beta_0 + beta_1 * exer_data[,2] + beta_2 * exer_data[,3] + beta_3 * exer_data[,4] + epsilon
  }
  else {
    beta_0 = 3
    beta_1 = 0
    beta_2 = 0
    beta_3 = 0 
    exer_data[,1] = beta_0 + beta_1 * exer_data[,2] + beta_2 * exer_data[,3] + beta_3 * exer_data[,4] + epsilon
  }
  exer_data
}
```


#####[1.2.2] Simulation for 2500 iterations (Significance & Non-Significance Model)

```{r}
birthday = 19770411
set.seed(birthday)

# Below code is to simulate the Significance Model 
model_significance = cbind(sigma_1_F=rep(0,no_loop),sigma_1_P=rep(0,no_loop),sigma_1_R_SQUARED=rep(0,no_loop),
                           sigma_5_F=rep(0,no_loop),sigma_5_P=rep(0,no_loop),sigma_5_R_SQUARED=rep(0,no_loop),
                           sigma_10_F=rep(0,no_loop),sigma_10_P=rep(0,no_loop),sigma_10_R_SQUARED=rep(0,no_loop))
extra_counter = 0

#Loop against each sigma to generate the anova for each simulation and populate the model_nsignificance
for(s in 1:length(sigma)){
  #print(paste("Sigma:",sigma[s]," extra_counter",extra_counter))
  for (i in 1:no_loop){
     mlr_data = mlr_sim(n,sigma[s],signifincance_ind = 1)
     null_model = lm(y~1,data = mlr_data)
     full_model = lm(y~x1+x2+x3,data = mlr_data)
     model_significance[i,s+extra_counter] = anova(null_model,full_model)$F[2]
     model_significance[i,s+extra_counter+1] = anova(null_model,full_model)$'Pr(>F)'[2]
     model_significance[i,s+extra_counter+2] = summary(full_model)$r.squared
  }
  extra_counter = extra_counter + 2
}
# Below code is to simulate the Non Significance Model 
model_nonsignificance = cbind(sigma_1_F=rep(0,no_loop),sigma_1_P=rep(0,no_loop),sigma_1_R_SQUARED=rep(0,no_loop),
                           sigma_5_F=rep(0,no_loop),sigma_5_P=rep(0,no_loop),sigma_5_R_SQUARED=rep(0,no_loop),
                           sigma_10_F=rep(0,no_loop),sigma_10_P=rep(0,no_loop),sigma_10_R_SQUARED=rep(0,no_loop))
extra_counter = 0
#Loop against each sigma to generate the anova for each simulation and populate the model_nonsignificance
for(s in 1:length(sigma)){
  print(paste("Sigma:",sigma[s]," extra_counter",extra_counter))
  for (i in 1:no_loop){
     mlr_data = mlr_sim(n,sigma[s],signifincance_ind = 0)
     null_model = lm(y~1,data = mlr_data)
     full_model = lm(y~x1+x2+x3,data = mlr_data)
     model_nonsignificance[i,s+extra_counter] = anova(null_model,full_model)$F[2]
     model_nonsignificance[i,s+extra_counter+1] = anova(null_model,full_model)$'Pr(>F)'[2]
     model_nonsignificance[i,s+extra_counter+2] = summary(full_model)$r.squared
  }
  extra_counter = extra_counter + 2
}
```

###[1.3] Results
As part of the results we will draw out the following

 - Simulate the MLR models (2 models) for 2500 times for a given value of σ
 - Plot the empirical distribution of the for the following 3 parameters 
     - F statistics
     - p-value
     - R2

#####[1.3.1] Empirical distibution for 2500 simulations (Sigma = 1)

```{r}
par(mfrow=c(1, 2))

hist(model_significance[,1],
     main = "Empirical Distribution of F [Significance]",
     cex.main = 0.8,
     xlab = "Simulated values of F",
     col = "darkolivegreen",
     border = "white",
     probability = TRUE
     )

hist(model_nonsignificance[,1],
     main = "Empirical Distribution of F [Non Significance]",
     cex.main = 0.8,     
     xlab = "Simulated values of F",
     col = "plum4",
     border = "white",
     probability = TRUE
     )
```

```{r}
par(mfrow=c(1, 2))

hist(model_significance[,2],
     main = "Empirical Distribution of P [Significance]",
     cex.main = 0.8,     
     xlab = "Simulated values of P",
     col = "sienna2",
     border = "white",
     probability = TRUE
     )

hist(model_nonsignificance[,2],
     main = "Empirical Distribution of P [Non Significance]",
     cex.main = 0.8,     
     xlab = "Simulated values of P",
     col = "slateblue1",
     border = "white",
     probability = TRUE
     )
```

```{r}
par(mfrow=c(1, 2))

hist(model_significance[,3],
     main = "Empirical Distribution of R Squared [Significance]",
     cex.main = 0.7,     
     xlab = "Simulated values of R Squared",
     col = "yellow",
     border = "darkolivegreen",
     probability = TRUE
     )

hist(model_nonsignificance[,3],
     main = "Empirical Distribution of R Squared [Non Significance]",
     cex.main = 0.7,     
     xlab = "Simulated values of R Squared",
     col = "tan",
     border = "white",
     probability = TRUE
     )
```

#####[1.3.2] Empirical distibution for 2500 simulations (Sigma = 5)

```{r}
par(mfrow=c(1, 2))

hist(model_significance[,4],
     main = "Empirical Distribution of F [Significance Model]",
     cex.main = 0.8,
     xlab = "Simulated values of F",
     col = "darkolivegreen",
     border = "white",
     probability = TRUE
     )

hist(model_nonsignificance[,4],
     main = "Empirical Distribution of F [Non Significance]",
     cex.main = 0.8,
     xlab = "Simulated values of F",
     col = "plum4",
     border = "white",
     probability = TRUE
     )
```

```{r}
par(mfrow=c(1, 2))

hist(model_significance[,5],
     main = "Empirical Distribution of P [Significance]",
     cex.main = 0.8,
     xlab = "Simulated values of P",
     col = "sienna2",
     border = "white",
     probability = TRUE
     )

hist(model_nonsignificance[,5],
     main = "Empirical Distribution of P [Non Significance]",
     cex.main = 0.8,
     xlab = "Simulated values of P",
     col = "slateblue1",
     border = "white",
     probability = TRUE
     )
```

```{r}
par(mfrow=c(1, 2))

hist(model_significance[,6],
     main = "Empirical Distribution of R Squared [Significance]",
     cex.main = 0.6,
     xlab = "Simulated values of R Squared",
     col = "yellow",
     border = "white",
     probability = TRUE
     )

hist(model_nonsignificance[,6],
     main = "Empirical Distribution of R Squared [Non Significance]",
     cex.main = 0.6,     
     xlab = "Simulated values of R Squared",
     col = "tan",
     border = "white",
     probability = TRUE
     )
```

#####[1.3.3] Empirical distibution for 2500 simulations (Sigma = 10)
```{r}
par(mfrow=c(1, 2))

hist(model_significance[,7],
     main = "Empirical Distribution of F [Significance Model]",
     cex.main = 0.6,
     xlab = "Simulated values of F",
     col = "darkolivegreen",
     border = "white",
     probability = TRUE     
     )


hist(model_nonsignificance[,7],
     main = "Empirical Distribution of F [Non Significance Model]",
     cex.main = 0.6,
     xlab = "Simulated values of F",
     col = "darkolivegreen",
     border = "white",
     probability = TRUE     
     )
```

```{r}
par(mfrow=c(1, 2))

hist(model_significance[,8],
     main = "Empirical Distribution of P [Significance Model]",
     cex.main = 0.6,
     xlab = "Simulated values of P",
     col = "darkgreen",
     border = "white",
     probability = TRUE     
     )

hist(model_nonsignificance[,8],
     main = "Empirical Distribution of P [Non Significance Model]",
     cex.main = 0.6,
     xlab = "Simulated values of P",
     col = "darkgreen",
     border = "white",
     probability = TRUE
     )
```

```{r}
par(mfrow=c(1, 2))

hist(model_significance[,9],
     main = "Empirical Distribution of R Squared [Significance Model]",
     cex.main = 0.6,
     xlab = "Simulated values of R Squared",
     col = "darkcyan",
     border = "white",
     probability = TRUE     
     )

hist(model_nonsignificance[,9],
     main = "Empirical Distribution of R Squared [Non Significance Model]",
     cex.main = 0.6,
     xlab = "Simulated values of R Squared",
     col = "darkcyan",
     border = "white",
     probability = TRUE     
     )
```

###[1.4] Discussion
As part of the introduction, we have the following discussion agenda
   - Do we know the true distribution of the following ?
     - F statistics
     - p-value
     - R square
   - How do the empirical distributions compare with the true distribution ?
   - How R2 is related to the sigma

#####[1.4.1]Do we know the true distribution
######[1.4.1.1]True distribution of the F Test
```{r}
curve(df(x, df1=3, df2=21),
      xlab = "Probability",
      ylab = "Frequency",
      main = "True Distribution of F Test(F Dstribution)",
      lwd=3,
      col = "darkgreen")
```

The True distribution of the F Tests shows as a **F distribution, which is right skewed distribution**

######[1.4.1.2] True distribution of the P value
```{r}
hist(pnorm(rnorm(25,mean = 3, sd = 1),mean=3,sd=1),
     xlab = "Probability",
     ylab = "Desnity",
     main = "True Distribution of P val (Uniform Distribution)",     
     breaks = 10,
     col = "orange",
     border = "white",
     probability = TRUE)
```

The True distribution of the P seems to have an **uniform distribution**

######[1.5.1.3]True distribution of the R squared
```{r}
curve(dbeta(x, (4-1)/2, (25-4)/2),
      xlab = "Probability",
      ylab = "Frequency",
      main = "True Distribution of R Squared (Beta distribution)",      
      lwd = 3,
      col = "palevioletred4")
```

The True distribution of R Squared is a **beta distribution (right skewed distribution)**, where the shape 1 and shape 2 parameter is of value (k-1)/2 and (n-k)/2, where k = number of esimator in the True model(which is 4,i.e $\beta_{0} , \beta_{1} , \beta_{2} , \beta_{3}$) and n = number of elements in the data (which is 25)

#####[1.4.2] How do the empirical distributions from the simulations compare to the true distributions?
######[1.4.2.1] Empirical distributions of F Test related to the True Distibution of F Test
```{r}
hist(model_nonsignificance[,1],
     main = "Empirical Distribution of F (Sigma = 1) [Non Significance Model]",
     cex.main = 0.6,
     xlab = "Simulated values of F",
     col = "darkolivegreen1",
     border = "white",
     probability = TRUE
     )
curve(df(x, df1=3, df2=21),
      lwd=3,
      col = "darkgreen",
      add = TRUE)
```

It seems the True distribution of the F test statistics is **matching** with the empirical distribution of the of the F Test. The dark green color shows the true distribution of the F test statistics, where as the histogram in light green color is the empirical distribution of the F test statistics obtained from the simulated values.

######[1.4.2.2]Empirical distributions of P value related to the True Distibution of P (Non Siginificance)
```{r}
par(mfrow=c(1, 2))

hist(model_nonsignificance[,2],
     main = "Empirical Distribution of P [Non Significance Model]",
     cex.main = 0.8,
     breaks = 10,     
     xlab = "Simulated values of P",
     col = "deepskyblue1",
     border = "white",
     probability = TRUE
     )

hist(pnorm(rnorm(25,mean = 3, sd = 1),mean=3,sd=1),
     xlab = "Probability",
     ylab = "Density",
     main = "True Distribution of P val (Uniform Distribution)",     
     cex.main = 0.8,
     breaks = 10,
     col = "orange",
     border = "white",
     probability = TRUE)
```

From the above plots the empirical distrubution of the P value (Left plot - Blue) is **matching** with the true distribution of the P value (right plot - orange), which is uniform distribution.

######[1.4.2.3]Empirical distributions of R squared related to the True Distibution of R Squared (Non Siginificance Model)
```{r}
hist(model_nonsignificance[,3],
     main = "Empirical Distribution of R Squared (Sigma = 1) [Non Significance Model]",
     cex.main = 0.8,
     xlab = "Simulated values of R Squared",
     col = "palevioletred1",
     border = "white",
     probability = TRUE
     )
curve(dbeta(x, (4-1)/2, (25-4)/2),
      lwd = 3,
      col = "palevioletred4",
      add = TRUE)
```

The empirical distribution is **matching** with the True distribution of the R Squared which is the **right skewed**, which follows **beta distribution**

######[1.4.3] How are R2 and sigma are related

```{r}
plot(model_significance[,3],col="dodgerblue",ylim=c(0,1),lty=1,xlab="R Squared Simulation Index",ylab="R Squared Value",main = "R Squared ~ Sigma")
points(model_significance[,6],col="green",lty=2)
points(model_significance[,9],col="red",lty=3)
legend("topright",legend=c("sigma=1","sigma=5","sigma=10"),col=c("dodgerblue","green","red"),lty=c(1,2,3))
```

**Description of how R2 and sigma are related**

 - From the above plot, it seems **R square and sigma are related**. 
 - **Higher the value of the sigma, lower the value of the R squared**. 
 - It seems prominent in case of sigma =1 (blue circles) for which the R squared value is bit higher as compared to the Sigma = 5 (green circles) and Sigma = 10 (red circles). 
 - The Red circles (Sigma = 10) have in the bottom with lowest R Squared value, where as green circles (Sigma = 5) have higher R squared values. 
 - Though there are few green circles (Sigma = 5) have lower R Squared same as of red circles (Sigma = 10) 


## [2] Simulation Study 2: Using RMSE for Selection?

###[2.1] Introduction
In this simulation project we will investigate the **procedure** for choosing the best model via test/train RMSE by simulating the following multiple regression model (**MLR**) 

\[ Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \beta_5 x_{i5} + \beta_6 x_{i6} + \epsilon_i.\]


where $\epsilon_i$ follows a Normal distribution of mean = 0 and Varience = $\sigma^2$

We have provided the value of the parameters as

 - β0 = 0
 - β1 = 5
 - β2 = -4
 - β3 = 1.6
 - β4 = -1.1
 - β5 = 0.7
 - β6 = 0.3

 - σ ∈ (1,2,4), **three distinct values of sigma**
 - n = 500
 
We need to evaluate 9 models (below), where for each model, we will calculate the RMSE for train data set (250 random observation) and test data set (250 random observation). We will simulate this for 1000 iteration for each value of σ, so there would be of 3000 simulation for each model, which equals to 3 X 3 X 3000 = **27000** for around 9 models

 - (Model -1 ) y ~ x1
 - (Model -2 ) y ~ x1 + x2
 - (Model -1 ) y ~ x1
 - (Model -2 ) y ~ x1 + x2
 - (Model -3 ) y ~ x1 + x2 + x3
 - (Model -4 ) y ~ x1 + x2 + x3 + x4
 - (Model -5 ) y ~ x1 + x2 + x3 + x4 + x5
 - (Model -6 ) y ~ x1 + x2 + x3 + x4 + x5 + x6, **the correct form of the model**
 - (Model -7 ) y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7
 - (Model -8 ) y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8
 - (Model -9 ) y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9


The Train/Test RMSE for each model is as below

RMSE(model, data) = $\sqrt{\dfrac{1}{n}\sum\limits_{t=1}^{n}(y_{i} - \hat{y}_i)^2}$

As per the above model, the correct model is the **Model -6**, we need to see whether by this methods/approach, we will able to get the correct model

####[2.1.2] Goal:

As part of this exercise, the following are the Goals to achieve

 - Simulate the MLR models for 1000 times for a given value of σ (1,2,4)
 - Plot 3 curves, each plot is against each value of sigma, shows what the Train RMSE and Test RMSE against the Model Size (Model 1,2,3,4,5,6).
 - From the 3 Plots, figure out whether the simulation helped to identify the best Model, i.e. **Model 6**
 - Discuss on the following points in the **Disucssion** section
 
     - Does the simulation method always select the correct model (i.e. **model 6**), or sometimes it select the other models
     - If we do the average of the Test RMSE, whether the simulation method selects the correct model (i.e. **Model 6**)
     - How does the level of noise (σ = 1,2 and 4) affects the Model selection.

 In order to achieve these goals, we have various methods in the form of R codes, markdown
 
###[2.2] Methods
Below is the code which will runs the 1000 simulations against each sigma (σ)

 - Will Create 9 matrix for Train Set which will stored the Train RMSE, where each columns holds the Train RMSE for each value of σ = 1,2 & 4
 - Will Create 9 matrix for Test Set which will stored the Test RMSE, where each columns holds the Train RMSE for each value of σ = 1,2 & 4
 - Will Create 9 matrix which stored the average Train Set RMSE from the Train Set RMESE, where each columns holds the Train RMSE for each value of σ = 1,2 & 4
 - Will Create 9 matrix which stored the average Test Set RMSE from the Test Set RMSE, where each columns holds the Train RMSE for each value of σ = 1,2 & 4

####[2.2.1] Code for Simulations
```{r}
birthday = 19770411
set.seed(birthday)

#Declaration
beta_0 = 0
beta_1 = 5
beta_2 = -4
beta_3 = 1.6
beta_4 = -1.1
beta_5 = 0.7
beta_6 = 0.3
n = 500
no_loop = 1000
sigma = c(1,2,4)
sig_data = read.csv("study_2.csv")

#Function to generate the sample data
mlr_sim = function(data,sd=1){
  epsilon = rnorm(n,mean = 0,sd = sd)
  data[,1] = beta_0 + beta_1 * data[,2] + beta_2 * data[,3] + beta_3 * data[,4] + beta_4 * data[,5] + beta_5 * data[,6] + beta_6 * data[,7] + epsilon
  data
}

#Defining the Train Matrix to stored the TRAIN RMSE
train_rmse_model1_mat = cbind(sigma_1=rep(0,no_loop),sigma_2=rep(0,no_loop),sigma_4=rep(0,no_loop))
train_rmse_model2_mat = cbind(sigma_1=rep(0,no_loop),sigma_2=rep(0,no_loop),sigma_4=rep(0,no_loop))
train_rmse_model3_mat = cbind(sigma_1=rep(0,no_loop),sigma_2=rep(0,no_loop),sigma_4=rep(0,no_loop))
train_rmse_model4_mat = cbind(sigma_1=rep(0,no_loop),sigma_2=rep(0,no_loop),sigma_4=rep(0,no_loop))
train_rmse_model5_mat = cbind(sigma_1=rep(0,no_loop),sigma_2=rep(0,no_loop),sigma_4=rep(0,no_loop))
train_rmse_model6_mat = cbind(sigma_1=rep(0,no_loop),sigma_2=rep(0,no_loop),sigma_4=rep(0,no_loop))
train_rmse_model7_mat = cbind(sigma_1=rep(0,no_loop),sigma_2=rep(0,no_loop),sigma_4=rep(0,no_loop))
train_rmse_model8_mat = cbind(sigma_1=rep(0,no_loop),sigma_2=rep(0,no_loop),sigma_4=rep(0,no_loop))
train_rmse_model9_mat = cbind(sigma_1=rep(0,no_loop),sigma_2=rep(0,no_loop),sigma_4=rep(0,no_loop))

#Defining the Test Matrix to stored the TESE RMSE
test_rmse_model1_mat = cbind(sigma_1=rep(0,no_loop),sigma_2=rep(0,no_loop),sigma_4=rep(0,no_loop))
test_rmse_model2_mat = cbind(sigma_1=rep(0,no_loop),sigma_2=rep(0,no_loop),sigma_4=rep(0,no_loop))
test_rmse_model3_mat = cbind(sigma_1=rep(0,no_loop),sigma_2=rep(0,no_loop),sigma_4=rep(0,no_loop))
test_rmse_model4_mat = cbind(sigma_1=rep(0,no_loop),sigma_2=rep(0,no_loop),sigma_4=rep(0,no_loop))
test_rmse_model5_mat = cbind(sigma_1=rep(0,no_loop),sigma_2=rep(0,no_loop),sigma_4=rep(0,no_loop))
test_rmse_model6_mat = cbind(sigma_1=rep(0,no_loop),sigma_2=rep(0,no_loop),sigma_4=rep(0,no_loop))
test_rmse_model7_mat = cbind(sigma_1=rep(0,no_loop),sigma_2=rep(0,no_loop),sigma_4=rep(0,no_loop))
test_rmse_model8_mat = cbind(sigma_1=rep(0,no_loop),sigma_2=rep(0,no_loop),sigma_4=rep(0,no_loop))
test_rmse_model9_mat = cbind(sigma_1=rep(0,no_loop),sigma_2=rep(0,no_loop),sigma_4=rep(0,no_loop))
train_rmse_mat = cbind(sigma_1=rep(0,9),sigma_2=rep(0,9),sigma_4=rep(0,9))
test_rmse_mat = cbind(sigma_1=rep(0,9),sigma_2=rep(0,9),sigma_4=rep(0,9))

#model counter variable to increase the index of the column in the matrix
model1_counter_all_sigma = 0 
model2_counter_all_sigma = 0 
model3_counter_all_sigma = 0 
model4_counter_all_sigma = 0 
model5_counter_all_sigma = 0 
model6_counter_all_sigma = 0 
model7_counter_all_sigma = 0 
model8_counter_all_sigma = 0 
model9_counter_all_sigma = 0 
  
#Looping through the each sigma to simulate for each 9 models
for(s in 1:length(sigma)){
  for(i in 1:no_loop){
    #epsilon = rnorm(n,mean = 0, sd = sigma[s])
    mlr_data=mlr_sim(sig_data,sd=sigma[s])
    
    #Split the Data into Train Data (Random)
    trn_idx = sample(1:nrow(mlr_data), 250)
    n_test = nrow(mlr_data) - length(trn_idx)

    #Model Creation    
    model1 = lm(y~x1,data=mlr_data[trn_idx,])
    model2 = lm(y~x1+x2,data=mlr_data[trn_idx,])
    model3 = lm(y~x1+x2+x3,data=mlr_data[trn_idx,])
    model4 = lm(y~x1+x2+x3+x4,data=mlr_data[trn_idx,])
    model5 = lm(y~x1+x2+x3+x4+x5,data=mlr_data[trn_idx,])
    model6 = lm(y~x1+x2+x3+x4+x5+x6,data=mlr_data[trn_idx,])
    model7 = lm(y~x1+x2+x3+x4+x5+x6+x7,data=mlr_data[trn_idx,])
    model8 = lm(y~x1+x2+x3+x4+x5+x6+x7+x8,data=mlr_data[trn_idx,])
    model9 = lm(y~x1+x2+x3+x4+x5+x6+x7+x8+x9,data=mlr_data[trn_idx,])
    
    #Predict the Test Data for th 9 model
    newdata_model1 = subset(mlr_data[-trn_idx,],select=c("x1"))
    newdata_model2 = subset(mlr_data[-trn_idx,],select=c("x1","x2"))
    newdata_model3 = subset(mlr_data[-trn_idx,],select=c("x1","x2","x3"))
    newdata_model4 = subset(mlr_data[-trn_idx,],select=c("x1","x2","x3","x4"))
    newdata_model5 = subset(mlr_data[-trn_idx,],select=c("x1","x2","x3","x4","x5"))
    newdata_model6 = subset(mlr_data[-trn_idx,],select=c("x1","x2","x3","x4","x5","x6"))
    newdata_model7 = subset(mlr_data[-trn_idx,],select=c("x1","x2","x3","x4","x5","x6","x7"))
    newdata_model8 = subset(mlr_data[-trn_idx,],select=c("x1","x2","x3","x4","x5","x6","x7","x8"))
    newdata_model9 = subset(mlr_data[-trn_idx,],select=c("x1","x2","x3","x4","x5","x6","x7","x8","x9"))    
    
    #Train RMSE
    train_rmse_model1 = sqrt(sum((mlr_data[trn_idx,]$y - predict(model1))^2) / length(trn_idx))
    train_rmse_model2 = sqrt(sum((mlr_data[trn_idx,]$y - predict(model2))^2) / length(trn_idx))
    train_rmse_model3 = sqrt(sum((mlr_data[trn_idx,]$y - predict(model3))^2) / length(trn_idx))
    train_rmse_model4 = sqrt(sum((mlr_data[trn_idx,]$y - predict(model4))^2) / length(trn_idx))
    train_rmse_model5 = sqrt(sum((mlr_data[trn_idx,]$y - predict(model5))^2) / length(trn_idx))
    train_rmse_model6 = sqrt(sum((mlr_data[trn_idx,]$y - predict(model6))^2) / length(trn_idx))
    train_rmse_model7 = sqrt(sum((mlr_data[trn_idx,]$y - predict(model7))^2) / length(trn_idx))
    train_rmse_model8 = sqrt(sum((mlr_data[trn_idx,]$y - predict(model8))^2) / length(trn_idx))
    train_rmse_model9 = sqrt(sum((mlr_data[trn_idx,]$y - predict(model9))^2) / length(trn_idx))
    train_rmse_model1_mat[i,s] = train_rmse_model1
    train_rmse_model2_mat[i,s] = train_rmse_model2
    train_rmse_model3_mat[i,s] = train_rmse_model3
    train_rmse_model4_mat[i,s] = train_rmse_model4
    train_rmse_model5_mat[i,s] = train_rmse_model5
    train_rmse_model6_mat[i,s] = train_rmse_model6
    train_rmse_model7_mat[i,s] = train_rmse_model7
    train_rmse_model8_mat[i,s] = train_rmse_model8
    train_rmse_model9_mat[i,s] = train_rmse_model9    
    
    #Test RMSE    
    test_rmse_model1 = sqrt(sum((mlr_data[-trn_idx,]$y - predict(model1,newdata=newdata_model1))^2) / n_test)
    test_rmse_model2 = sqrt(sum((mlr_data[-trn_idx,]$y - predict(model2,newdata=newdata_model2))^2) / n_test)
    test_rmse_model3 = sqrt(sum((mlr_data[-trn_idx,]$y - predict(model3,newdata=newdata_model3))^2) / n_test)
    test_rmse_model4 = sqrt(sum((mlr_data[-trn_idx,]$y - predict(model4,newdata=newdata_model4))^2) / n_test)
    test_rmse_model5 = sqrt(sum((mlr_data[-trn_idx,]$y - predict(model5,newdata=newdata_model5))^2) / n_test)
    test_rmse_model6 = sqrt(sum((mlr_data[-trn_idx,]$y - predict(model6,newdata=newdata_model6))^2) / n_test)
    test_rmse_model7 = sqrt(sum((mlr_data[-trn_idx,]$y - predict(model7,newdata=newdata_model7))^2) / n_test)
    test_rmse_model8 = sqrt(sum((mlr_data[-trn_idx,]$y - predict(model8,newdata=newdata_model8))^2) / n_test)
    test_rmse_model9 = sqrt(sum((mlr_data[-trn_idx,]$y - predict(model9,newdata=newdata_model9))^2) / n_test)
    test_rmse_model1_mat[i,s] = test_rmse_model1
    test_rmse_model2_mat[i,s] = test_rmse_model2
    test_rmse_model3_mat[i,s] = test_rmse_model3
    test_rmse_model4_mat[i,s] = test_rmse_model4
    test_rmse_model5_mat[i,s] = test_rmse_model5
    test_rmse_model6_mat[i,s] = test_rmse_model6
    test_rmse_model7_mat[i,s] = test_rmse_model7
    test_rmse_model8_mat[i,s] = test_rmse_model8
    test_rmse_model9_mat[i,s] = test_rmse_model9

    #Increase the Model Counter
    model1_counter_all_sigma = model1_counter_all_sigma + 1
    model2_counter_all_sigma = model2_counter_all_sigma + 1
    model3_counter_all_sigma = model3_counter_all_sigma + 1 
    model4_counter_all_sigma = model4_counter_all_sigma + 1 
    model5_counter_all_sigma = model5_counter_all_sigma + 1
    model6_counter_all_sigma = model6_counter_all_sigma + 1 
    model7_counter_all_sigma = model7_counter_all_sigma + 1
    model8_counter_all_sigma = model8_counter_all_sigma + 1
    model9_counter_all_sigma = model9_counter_all_sigma + 1
  }
}

#Print the message of Number of times the each model (1/2/3/4/5/6/7/8/9) is being trained for each value of sigma
print (paste("Number of times the model : 1 trained for alpha:",sigma[s]," is: ",model1_counter_all_sigma))
print (paste("Number of times the model : 2 trained for alpha:",sigma[s]," is: ",model2_counter_all_sigma))
print (paste("Number of times the model : 3 trained for alpha:",sigma[s]," is: ",model3_counter_all_sigma))
print (paste("Number of times the model : 4 trained for alpha:",sigma[s]," is: ",model4_counter_all_sigma))
print (paste("Number of times the model : 5 trained for alpha:",sigma[s]," is: ",model5_counter_all_sigma))
print (paste("Number of times the model : 6 trained for alpha:",sigma[s]," is: ",model6_counter_all_sigma))  
print (paste("Number of times the model : 7 trained for alpha:",sigma[s]," is: ",model7_counter_all_sigma))  
print (paste("Number of times the model : 8 trained for alpha:",sigma[s]," is: ",model8_counter_all_sigma))  
print (paste("Number of times the model : 9 trained for alpha:",sigma[s]," is: ",model9_counter_all_sigma))  
  
#Average out the Train RMSE / Test RMSE for each value of sigma
for(s in 1:length(sigma)){
  train_rmse_mat[1,s] = mean(train_rmse_model1_mat[,s])
  train_rmse_mat[2,s] = mean(train_rmse_model2_mat[,s])
  train_rmse_mat[3,s] = mean(train_rmse_model3_mat[,s])
  train_rmse_mat[4,s] = mean(train_rmse_model4_mat[,s])
  train_rmse_mat[5,s] = mean(train_rmse_model5_mat[,s])
  train_rmse_mat[6,s] = mean(train_rmse_model6_mat[,s])
  train_rmse_mat[7,s] = mean(train_rmse_model7_mat[,s])
  train_rmse_mat[8,s] = mean(train_rmse_model8_mat[,s])
  train_rmse_mat[9,s] = mean(train_rmse_model9_mat[,s])  
  
  test_rmse_mat[1,s] = mean(test_rmse_model1_mat[,s])
  test_rmse_mat[2,s] = mean(test_rmse_model2_mat[,s])
  test_rmse_mat[3,s] = mean(test_rmse_model3_mat[,s])
  test_rmse_mat[4,s] = mean(test_rmse_model4_mat[,s])
  test_rmse_mat[5,s] = mean(test_rmse_model5_mat[,s])
  test_rmse_mat[6,s] = mean(test_rmse_model6_mat[,s])
  test_rmse_mat[7,s] = mean(test_rmse_model7_mat[,s])
  test_rmse_mat[8,s] = mean(test_rmse_model8_mat[,s])
  test_rmse_mat[9,s] = mean(test_rmse_model9_mat[,s])
}
```
###[2.3] Results
As we have discussed in the Introduction on the aspect of **Goal**, we have to producded plots to support these Goals

 - Simulate the MLR models for 1000 times for a given value of σ
 - Plot 3 curves, each plot is against each sigma, shows what the Train RMSE and Test RMSE against the Model Size (Model1,2,3,4,5,6).
 
####[2.3.1] Train/Test RMSE against 3 sigma values ((1,2,4), for each model (1/2/3.../7/8/9)
```{r}
plot(seq(1,9), train_rmse_mat[,1], type="o", col="blue", pch="o", lty=1,main="Train, Test MSE for Sigma = 1",xlab = "MODEL SIZE",ylab = "RMSE")
points(seq(1,9), test_rmse_mat[,1], col="red", pch=8)
lines(seq(1,9), test_rmse_mat[,1], col="red",lty=2)
legend("topright",legend=c("train rmse","test rmse"), col=c("blue","red"),pch=c("o","*"),lty=c(1,2), ncol=1)
```

The above plot shows the Train RMSE and Test RMSE for sigma = 1, is being very close to each other. As the model grow, both the Train / Test RMSE falls, however the tipping point is the **Model 6**, the test RMSE increases after the Model 6

```{r}
plot(seq(1,9), train_rmse_mat[,2], type="o", col="blue", pch="o", lty=1,main="Train, Test MSE for Sigma = 2",xlab = "MODEL SIZE",ylab = "RMSE")
points(seq(1,9), test_rmse_mat[,2], col="red", pch=8)
lines(seq(1,9), test_rmse_mat[,2], col="red",lty=2)
legend("topright",legend=c("train rmse","test rmse"), col=c("blue","red"),pch=c("o","*"),lty=c(1,2), ncol=1)
```

The above plot shows the Train RMSE and Test RMSE for sigma = 2, is being **very close to each other till model 6**, after model 6 they are ditant apart to each other. Also As the model grow, both the Train / Test RMSE falls, however the tipping point is the **Model 6**, after Model 6, the Test RMSE increases

```{r}
plot(seq(1,9), train_rmse_mat[,3], type="o", col="blue", pch="o", lty=1,main="Train, Test MSE for Sigma = 4",xlab = "MODEL SIZE",ylab = "RMSE")
points(seq(1,9), test_rmse_mat[,3], col="red", pch=8)
lines(seq(1,9), test_rmse_mat[,3], col="red",lty=2)
legend("topright",legend=c("train rmse","test rmse"), col=c("blue","red"),pch=c("o","*"),lty=c(1,2), ncol=1)
```

The above plot shows the Train RMSE and Test RMSE for sigma = 4, is being **very close to each other till model 2**, **after model 2** they are distant apart to each other and distance between them **increase as the model grows**. Also as the model grow, both the Train / Test RMSE falls, however the tipping point is the **Model 6**, after the Model 6, the Test RMSE value increases

###[2.4] Discussion
We will discuss on the following points, as we mentioned in the **Introduction** section

   - Does the simulation method always select the correct model (i.e. **model 6**), or sometimes it select the other models
   - If we do the average of the Test RMSE, whether the simulation method selects the correct model (i.e. **Model 6**)
   - How does the level of noise (σ = 1,2 and 4) affects the Model selection.

####[2.4.1] Does the simulation method always select the correct model?
```{r}
plot(test_rmse_model1_mat[,1],col="dodgerblue",ylim=(c(0.8,3)),pch=1,lty=1,xlab = "Index", ylab = "RMSE")
points(test_rmse_model2_mat[,1], col="orangered",ylim=(c(0.8,3)),pch=1,lty=2)
points(test_rmse_model3_mat[,1], col="blue4",ylim=(c(0.8,3)),pch=1,lty=3)
points(test_rmse_model4_mat[,1], col="limegreen",ylim=(c(0.8,3)),pch=1,lty=3)
points(test_rmse_model5_mat[,1], col="orange",ylim=(c(0.8,3)),pch=1,lty=4)
points(test_rmse_model6_mat[,1], col="darkorchid",ylim=(c(0.8,3)),pch=6,lty=5)
points(test_rmse_model7_mat[,1], col="violetred",ylim=(c(0.8,3)),pch=1,lty=6)
points(test_rmse_model8_mat[,1], col="tan3",ylim=(c(0.8,3)),pch=1,lty=7)
points(test_rmse_model9_mat[,1], col="deeppink",ylim=(c(0.8,3)),pch=1,lty=8)
legend("topright",legend=c("Model1","Model2","Model3","Model4","Model5","Model6","Model7","Model8","Model9"),col=c("dodgerblue","orangered","blue4","limegreen","orange","darkorchid","violetred","tan3","deeppink"),lty=c(1,2,3,4,5,6,7,8,9))
```

**The above plots the RMSE of all model created from the simulation for the Sigma = 1**

 The correct model should be the one which should have the **lowest RMSE**. From the above plots (which plots against all the 1000 RMSE of simulation) it shows that the Model 1(blue color) and Model 2(red color) have higher RMSE and distinctly far from the other model (Model 3/4/5/6/7/8/9), lets ignore the Model 1 and Model 2. Let's explore the other models as below 

```{r}
plot(test_rmse_model3_mat[,1], col="dodgerblue",ylim=(c(0.9,1.3)),pch=1,lty=3,xlab="Indx",ylab="RMSE")
points(test_rmse_model4_mat[,1], col="limegreen",ylim=(c(0.9,1.3)),pch=1,lty=3)
points(test_rmse_model5_mat[,1], col="orange",ylim=(c(0.9,1.3)),pch=1,lty=4)
points(test_rmse_model6_mat[,1], col="darkorchid",ylim=(c(0.9,1.3)),pch=8,lty=5)
points(test_rmse_model7_mat[,1], col="violetred",ylim=(c(0.9,1.3)),pch=1,lty=6)
points(test_rmse_model8_mat[,1], col="tan3",ylim=(c(0.9,1.3)),pch=1,lty=7)
points(test_rmse_model9_mat[,1], col="deeppink",ylim=(c(0.9,1.3)),pch=1,lty=8)
legend("topright",legend=c("Model3","Model4","Model5","Model6","Model7","Model8","Model9"),col=c("blue4","limegreen","orange","darkorchid","violetred","tan3","deeppink"),pch=c(1,1,1,8,1,1),lty=c(3,4,5,6,7,8,9))
```

**The above model plots the RMSE of models (3/4/5/6/7/8/9) created from the simulation for the Sigma = 1**

From the above plot, its clear that the **Model 6 not always** (marked in **darkorchid**) selected as the **best model**, because doesn't shows the RMSE is lowest among the other models(2/3/4/5/7/8/9). There are many instances, where the **Model 6** RMSE is higher than that of other models. Also though, there are instances where the **Model 6** RMSE is the lowest among other models. However other models also have the RMSE in range with the **model 6**. Hence, if we consider all the 1000 simulated RMSE, **Model 6 is not the best model always being selected**. Let's take a loot at the average plot of RMSE of all the models and plot


####[2.4.4] Average of the Test RMSE, methods selects the correct model (i.e. **Model 6**)?
```{r}
plot(seq(1,9), test_rmse_mat[,1], type="o", col="chartreuse4", pch=8, lty=1,main="Sigma = 1",xlab = "MODEL SIZE",ylab = "RMSE")
points(6, min(test_rmse_mat[,1]), col="chartreuse4",pch=16,cex=3) #This is to highlight the point where the RMSE is minimum for sigma = 1
```

The above plot shows the **model 6** has the lowest RMSE which is around `r min(test_rmse_mat[,1])`, which runs on average RMSE for **1000 simulation** over **9 models** for the **sigma = 1**. In this methods (averaging) shows **model 6** selects as the best model

```{r}
plot(seq(1,9), test_rmse_mat[,2], type="o", col="blue4", pch="*", lty=1,main="Sigma = 2",xlab = "MODEL SIZE",ylab = "RMSE")
points(6, min(test_rmse_mat[,2]), col="blue4",pch=16,cex=3) #This is to highlight the point where the RMSE is minimum for sigma = 2
```

The above plot shows the **model 6** has the lowest RMSE which is around `r min(test_rmse_mat[,2])`, which runs on average RMSE for **1000 simulation** over **9 models** for the **sigma = 1**. In this methods (averaging) shows **model 6** selects as the best model


```{r}
plot(seq(1,9), test_rmse_mat[,3], type="o", col="red", pch="*", lty=1,main="Sigma = 4",xlab = "MODEL SIZE",ylab = "RMSE")
points(6, min(test_rmse_mat[,3]), col="red",pch=16,cex=3) #This is to highlight the point where the RMSE is minimum for sigma = 4
```

The above plot shows the **model 6** has the lowest RMSE which is around `r min(test_rmse_mat[,3])`, which runs on average RMSE for **1000 simulation** over **9 models** for the **sigma = 1**. In this methods (averaging) shows **model 6** selects as the best model


```{r}
plot(seq(1,9), test_rmse_mat[,3], col="red",lty=1,ylim=c(0.5,5),xlab="MODEL SIZE",ylab="RMSE")
lines(seq(1,9), test_rmse_mat[,3], col="red",lty=1)
points(6, min(test_rmse_mat[,3]), col="red",pch=16,cex=3) #This is to highlight the point where the RMSE is minimum for sigma = 4
points(seq(1,9), test_rmse_mat[,2], col="blue4",lty=2)
lines(seq(1,9), test_rmse_mat[,2], col="blue4",lty=2)
points(6, min(test_rmse_mat[,2]), col="blue4",pch=16,cex=3) #This is to highlight the point where the RMSE is minimum for sigma = 2
points(seq(1,9), test_rmse_mat[,1], col="chartreuse4",lty=3)
lines(seq(1,9), test_rmse_mat[,1], col="chartreuse4",lty=3)
points(6, min(test_rmse_mat[,1]), col="chartreuse4",pch=16,cex=3) #This is to highlight the point where the RMSE is minimum for sigma = 1
legend("topright",legend=c("sigma = 4","sigma = 2","simga = 1"),col=c("red","blue4","chartreuse4"),lty=c(1,2,3))
```

The above plots shows the comparision of the average RMSE of all the 9 models for respective 3 sigma values (1, 2 and 4). The big filled circle points shows the lowest of the RMSE against the Model Size. From the comparision of the plot, it shows that **lowest RMSE** is from the **sigma =1**, also noted that **higher the value of sigma, higher will be the Test RMSE**. Hence to lower the RMSE for the test, we need to choose the lower value of noise (i.e. sigma) 


## [3] Simulation Study 3, Power

###[3.1] Introduction
In this simulation project we will investigate the **power** of the signifincance of the regression test for the simple linear regression (**SLR**) model

$H_0 : \beta_{0} = 0 Vs H_1: \beta_{1} \neq 0$

Where, Power is the probability of rejecting the null hypothesis when the null is not true, that is, the alternative is true and $\beta_{1}$ is non-zero.

We will do the simulation of the Simple linear regression (**SLR**) of the model \[ Y_i = \beta_0 + \beta_1 x_{i1} + \epsilon_i.\]. 

For the simplicity we will make $\beta_0$ = 0, thus $\beta_1$ is essentially controlling the amount of signal. We will be considering the following for the different signals , noises and sample sizes

 - β1 ∈ (−2,−1.9,−1.8,…,−0.1,0,0.1,0.2,0.3,…1.9,2) , **which is 41 values**
 - σ ∈ (1,2,4), **three distinct values of sigma**
 - n ∈(10,20,30), **three distinct values of n (sample size)**
 
We will hold the value if the $\alpha$ as **0.05**, while validing the hypothesis. For the generation of the sample data, will be using the **seq(0, 5, length = n)** for different values of the sample size (i.e., n)

For each possible combination of $\beta_{1}$ and $\alpha$, we will simulate 1000 times to perform the significance of linear regression (**SLR**) and following formula will be using to measure the Power

$\hat{Power}$ = $\hat{P}$[Reject $H_0$, $H_1$ True] = $\dfrac{\#Test Rejected}{\#Simulations}$

####[3.1.1] Goal

As part of this exercise, the following are the Goals to achieve

 - Simulate the SLR models for 1000 times for a given value of σ, n , β1 and stored the Power you get
 - Plot the Power Curve against the signal strength (β1) for different value of n, plot this kind of plot for each value of σ
 - Discuss with relevant plots, what are the impact of σ, n , β1 on the Power, as following 
     - What's the impact of σ on Power
     - What's the impact of n on Power
     - What's the impact of β1 on Power
 - As we have simulated for the 1000 iterations, would it be suffice ? or more simulations have more important informations to revealed?

 In order to achieve these goals, we have various methods in the form of R codes, markdown
 
###[3.2] Methods
Below are the methods proposed which will runs the 1000 simulations against β1, σ and n. After each simulations,

 - The power value is being agregated and stored in the matrix model_sim
 - The 1st column of the row of the matix is the β1 value
 - 2nd, 3rd and 4th column of matrix = model_sim, stores the power values for sigma = 1, for the sample size 10, 20 and 30 respectively
 - 5th, 6th and 7th column of matrix = model_sim, stores the power values for sigma = 2, for the sample size of 10, 20 and 30 respectively
 - 8th, 9th and 10th column of matrix = model_sim, stores the power values for sigma = 5, for the sample size of 10, 20 and 30 respectively
 
Once the matrix model_sim is populated, we will be using the matix to have different plots to address different discussion

####[3.2.1] Code for Simulations
```{r}
birthday = 19770411
set.seed(birthday)

alpha = 0.05
beta_1 = seq(from=-2,to=2,by=0.1)
sigma = c(1,2,4)
n = c(10,20,30)
no_loop = 1000

#simple linear regression(SLR) Function to create sample data
slr_sim = function(n_in,sd=1,beta_1_in=-2){
  x_values = seq(0, 5, length = n_in)
  epsilon = rnorm(n_in,mean = 0,sd = sd)
  y = beta_1_in * x_values + epsilon
  data.frame(predictor = x_values,response=y)
}

#Matrix to store the Power of the simulated models
model_sim = cbind(beta_1_sim_val=rep(0,length(beta_1)),n_10_sigma_1=rep(0,length(beta_1)),n_20_sigma_1=rep(0,length(beta_1)),n_30_sigma_1=rep(0,length(beta_1)),n_10_sigma_2=rep(0,length(beta_1)),n_20_sigma_2=rep(0,length(beta_1)),n_30_sigma_2=rep(0,length(beta_1)),n_10_sigma_4=rep(0,length(beta_1)),n_20_sigma_4=rep(0,length(beta_1)),n_30_sigma_1=rep(0,length(beta_1)))

model_sim[,1] = seq(from=-2,to=2,by=0.1)
extra_counter = 0

#Loop to iterate over Sigma, number of samples, beta_1 values and 1000 simulations to extract and store the Power values
run_counter = 0
for(s in 1:length(sigma)){ #sigma = c(1,2,4)
  for (n_s in 1:length(n)){ #n = c(10,20,30)
    for (b in 1:length(beta_1)){ # beta_1 = seq(from=-2,to=2,by=0.1)
      tot_sum_signifincace_sim = 0
      run_counter = run_counter + 1      
      for(i in 1:no_loop){
        slr_data = slr_sim(n_in = n[n_s], sd = sigma[s], beta_1_in = beta_1[b] )
        model = lm(response~predictor, data = slr_data)
        model_p_val = summary(model)$coefficient[2,4]
        tot_sum_signifincace_sim = tot_sum_signifincace_sim + ifelse(model_p_val<alpha,1,0)
      }
      power_sim_beta = tot_sum_signifincace_sim / no_loop
      model_sim[b,s+n_s+extra_counter] = power_sim_beta    
      print (paste(run_counter," Sigma:",sigma[s]," n:",n[n_s]," beta_1:",beta_1[b]," Signifiance_Sum:",tot_sum_signifincace_sim, " Power:",power_sim_beta))
    }
  }
  extra_counter = extra_counter + 2
}
```

###[3.3] Results
As we have discussed in the Introduction on the aspect of **Goal**, we have to producded plots to support these Goals

 - Plot the Power Curve against the signal strength (β1) for different value of n, plot this kind of plot for each value of σ


```{r}
#Below code plot the effect of Signal (β1) against the Power for noise of sigma = 1 against all sample size (10,20 and 30)
plot(model_sim[,1],model_sim[,2], type="o", col="dodgerblue", pch=1, lty=1,main="Sigma = 1",xlab = "beta_1",ylab = "Power")
points(model_sim[,1], model_sim[,3], col="red", pch=8,lty=2)
lines(model_sim[,1], model_sim[,3], col="red",lty=2)
points(model_sim[,1], model_sim[,4], col="darkorchid", pch=16,lty=3)
lines(model_sim[,1], model_sim[,4], col="darkorchid",lty=3)
legend("topright",legend=c("n = 10","n = 20"," n = 30"),col=c("dodgerblue","red","darkorchid"),pch=c(1,8,16),lty=c(1,2,3))
```

The above plot depicts the following for sigma = 1

 - The power is 1 (i.e. # rejected hypothesis = # simulation) for **beta_1 between -2 and -1** for all sample size (10,20 and 30)
 - As beta_1 increases after -1 and before 0, **power decreases rapidly towards 0**, means number of rejected hypothesis becomes small (failed to reject hypothesis becomes larger) 
 - Again as beta_1 increases after 0, the **power increases**, i.e. number of rejected hypothesis is large.
 - Dropping of power nearly or after -1 depends on the sample size, more the sample size (30 > 20 > 10) power becomes 1 (i.e. # reject hypothesis is higher)



#####[3.3.1] Effect of Signal (β1) against Power
```{r}
plot(model_sim[,1],model_sim[,5], type="o", col="dodgerblue", pch=1, lty=1,main="Sigma = 2",xlab = "beta_1",ylab = "Power")
points(model_sim[,1], model_sim[,6], col="red", pch=8,lty=2)
lines(model_sim[,1], model_sim[,6], col="red",lty=2)
points(model_sim[,1], model_sim[,7], col="darkorchid", pch=16,lty=3)
lines(model_sim[,1], model_sim[,7], col="darkorchid",lty=3)
legend("topright",legend=c("n = 10","n = 20"," n = 30"),col=c("dodgerblue","red","darkorchid"),pch=c(1,8,16),lty=c(1,2,3))
```

The above plot depicts the following for the sigma = 2

 - The power is 1 (i.e. # rejected hypothesis = # simulation) for beta_1 between till -1.5 for sample size 20 and 30. However lower the sample size with higher noise, the power decreases rapidly (i.e., # of hypothesis rejectes is small)
 - Once beta_1 increase after 0, power increases very rapidly for higher sample size, however slow for smaller sample size
 - As noise level increases (Sigma =2), the rate at which the power increases/ decreases also lower. So if we compare the power rate increases / decreases between sigma = 1 versus sigma = 2, it founds that power rate increase/decreases is much faster rate in sigma = 1 than that of sigma = 2
 - The distance between the power increase/decrease for different sample size (n=10/20 and 30) is wider for higher noise level (sigma = 2) than that of smaller noise level(sigma =1), which means reject the hypothesis is higher for higher sigma and even higher for lower sample size


#####[3.3.2] Below code plot the effect of Signal (β1) against the Power for noise of sigma = 4 against all sample size (10,20 and 30)
```{r}
plot(model_sim[,1],model_sim[,8], type="o", col="dodgerblue", pch=1, lty=1,main="Sigma = 4",xlab = "beta_1",ylab = "Power",ylim=c(0,1))
points(model_sim[,1], model_sim[,9], col="red", pch=8,lty=2)
lines(model_sim[,1], model_sim[,9], col="red",lty=2)
points(model_sim[,1], model_sim[,10], col="darkorchid", pch=16,lty=3)
lines(model_sim[,1], model_sim[,10], col="darkorchid",lty=3)
legend("topright",legend=c("n = 10","n = 20"," n = 30"),col=c("dodgerblue","red","darkorchid"),pch=c(1,8,16),lty=c(1,2,3))
```

The above plot depicts the following for tsigma = 4

 - The power seems lower at the initial stage of beta_1 itself, for the lower sample size (10) and decrease very fast until 0. However though it decreases very fast for higher sample size(n=20, 30), however relatively slow as compared to lower sample size (n=10)
 - The above point still valid for the case of increase of beta_1 beyond point 0, where power increases rapidly for higher sample size (30 and then 30) as compared to lower sample size
 - The distance between the power increase/decrease for different sample size (n=10/20 and 30) is wider for higher noise level (sigma = 4) than that of sigma =1 & 2, means rejecte the null hypothesis is higher for higher simga and even higher for lower sample size


###[3.4] Discussion
Now as part of the **goal**, let's discuss the what's the impact of β1, n and σ with respect to power. We will be plotting different plots to conclude the summary of the impact

####[3.4.1] Effect of sigma(1,2,4) on the Power **
```{r}
plot(model_sim[,1],model_sim[,2], type="o", col="dodgerblue", pch=1, lty=1,main="Effect of sigma (1,2,4) on the Power",xlab = "beta_1",ylab = "Power")
points(model_sim[,1], model_sim[,3], col="dodgerblue", pch=1,lty=1)
lines(model_sim[,1], model_sim[,3], col="dodgerblue",lty=1)
points(model_sim[,1], model_sim[,4], col="dodgerblue", pch=1,lty=1)
lines(model_sim[,1], model_sim[,4], col="dodgerblue",lty=1)

points(model_sim[,1],model_sim[,5], type="o", col="green", pch=1, lty=2)
lines(model_sim[,1],model_sim[,5], type="o", col="green", pch=1, lty=2)
points(model_sim[,1], model_sim[,6], col="green", pch=1,lty=2)
lines(model_sim[,1], model_sim[,6], col="green",lty=2)
points(model_sim[,1], model_sim[,7], col="green", pch=1,lty=2)
lines(model_sim[,1], model_sim[,7], col="green",lty=2)

points(model_sim[,1],model_sim[,8], type="o", col="red", pch=1, lty=3)
lines(model_sim[,1],model_sim[,8], type="o", col="red", pch=1, lty=3)
points(model_sim[,1], model_sim[,9], col="red", pch=1,lty=3)
lines(model_sim[,1], model_sim[,9], col="red",lty=3)
points(model_sim[,1], model_sim[,10], col="red", pch=1,lty=3)
lines(model_sim[,1], model_sim[,10], col="red",lty=3)
legend("topright",legend=c("sigma=1,n=10,20,30","sigma=2,n=10,20,30","sigma=3,n=10,20,30"),col=c("dodgerblue","green","darkorchid","red"),pch=c(1,1,1),lty=c(1,2,3))
```

**Below are the observations on the effect of σ on power**

 - **Higher the sigma (σ),lower is the Power**
 - It clearly visible that all the blue line (sigma=1) have higher power than that of green line (sigma = 2) & red line (sigma = 4) for any particular value of β1 and sample size. That means higher the value of the sigma (σ) = number of reject the hypothesis will be lower and lower the value of sigma (σ) = number of reject the null hypothesis becomes higher.

#####[3.4.2] Effect of n(10,20,30) on the Power **
```{r}
par(mfrow=c(2, 2))

plot(model_sim[,1],model_sim[,2], type="o", col="deeppink", pch=1, lty=1,main="Sigma=1",xlab = "beta_1",ylab = "Power")
points(model_sim[,1], model_sim[,3], col="darkolivegreen4", pch=1,lty=2)
lines(model_sim[,1], model_sim[,3], col="darkolivegreen4",lty=2)
points(model_sim[,1], model_sim[,4], col="blueviolet", pch=1,lty=3)
lines(model_sim[,1], model_sim[,4], col="blueviolet",lty=3)
legend("topright",legend=c("n=10","n=20","n=30"),col=c("deeppink","darkolivegreen4","blueviolet"),pch=c(1,1,1),lty=c(1,2,3))

plot(model_sim[,1],model_sim[,5], type="o", col="deeppink", pch=1, lty=1,main="Sigma=2",xlab = "beta_1",ylab = "Power")
#points(model_sim[,1],model_sim[,5], type="o", col="deeppink", pch=1, lty=1)
lines(model_sim[,1],model_sim[,5], type="o", col="deeppink", pch=1, lty=1)
points(model_sim[,1], model_sim[,6], col="darkolivegreen4", pch=1,lty=2)
lines(model_sim[,1], model_sim[,6], col="darkolivegreen4",lty=2)
points(model_sim[,1], model_sim[,7], col="blueviolet", pch=1,lty=3)
lines(model_sim[,1], model_sim[,7], col="blueviolet",lty=3)
legend("topright",legend=c("n=10","n=20","n=30"),col=c("deeppink","darkolivegreen4","blueviolet"),pch=c(1,1,1),lty=c(1,2,3))

plot(model_sim[,1],model_sim[,8], type="o", col="deeppink", pch=1, lty=1,main="Sigma=4",xlab = "beta_1",ylab = "Power",ylim=c(0,1))
#points(model_sim[,1],model_sim[,8], type="o", col="deeppink", pch=1, lty=1)
lines(model_sim[,1],model_sim[,8], type="o", col="deeppink", pch=1, lty=1)
points(model_sim[,1], model_sim[,9], col="darkolivegreen4", pch=1,lty=2)
lines(model_sim[,1], model_sim[,9], col="darkolivegreen4",lty=2)
points(model_sim[,1], model_sim[,10], col="blueviolet", pch=1,lty=3)
lines(model_sim[,1], model_sim[,10], col="blueviolet",lty=3)
legend("topright",legend=c("n=10","n=20","n=30"),col=c("deeppink","darkolivegreen4","blueviolet"),pch=c(1,1,1),lty=c(1,2,3))
```

**Below are the observations on the effect of sample size (n = 10,20 and 30) on the beta_1**

 - From the abobe 3 plots, for different value of sigma (σ), **Higher the sample size (n=30),higher is the power**
 - It clearly visible that for all the 3 plots (against 3 different sigma (σ)), the blueviolet (n = 30) lines have higher than that of the rest 2 lines (green,n = 20 and deep pink, n= 10). That is higher the sample size, reject the hypothesis becomes higher

#####[3.4.3] Effect of n(10,20,30) on the Power **
```{r}
plot(model_sim[,1],model_sim[,2], type="o", col="dodgerblue", pch=1, lty=1,main="Sigma = 1,2,4 & n = 10,20,30",xlab = "beta_1",ylab = "Power")
points(model_sim[,1], model_sim[,3], col="red", pch=2,lty=2)
lines(model_sim[,1], model_sim[,3], col="red",lty=2)
points(model_sim[,1], model_sim[,4], col="tan1", pch=3,lty=3)
lines(model_sim[,1], model_sim[,4], col="tan1",lty=3)

points(model_sim[,1],model_sim[,5], type="o", col="yellow", pch=4, lty=4)
lines(model_sim[,1],model_sim[,5], type="o", col="yellow", pch=4, lty=4)

points(model_sim[,1], model_sim[,6], col="deeppink", pch=5,lty=5)
lines(model_sim[,1], model_sim[,6], col="deeppink",lty=5)
points(model_sim[,1], model_sim[,7], col="darkolivegreen4", pch=6,lty=6)
lines(model_sim[,1], model_sim[,7], col="darkolivegreen4",lty=6)

points(model_sim[,1],model_sim[,8], type="o", col="darkgreen", pch=7, lty=7)
lines(model_sim[,1],model_sim[,8], type="o", col="darkgreen", pch=7, lty=7)

points(model_sim[,1], model_sim[,9], col="darkblue", pch=8,lty=8)
lines(model_sim[,1], model_sim[,9], col="darkblue",lty=8)
points(model_sim[,1], model_sim[,10], col="blueviolet", pch=9,lty=9)
lines(model_sim[,1], model_sim[,10], col="blueviolet",lty=9)

legend("topright",legend=c("sigma=1,n=10","sigma=1,n=20","sigma=1,n=30","sigma=2,n=10","sigma=2,n=20","sigma=2,n=30","sigma=3,n=10","sigma=3,n=20","sigma=3,n=30"),col=c("dodgerblue","red","darkorchid","yellow","deeppink","darkolivegreen4","darkgreen","darkblue","blueviolet"),pch=c(1,2,3,4,5,6,7,8,9),lty=c(1,2,3,4,5,6,7,8,9))
```

If we combine all the plots for all the sample size beta(β1), sample size(n) and sigma(σ), from the above figure, **below are the observations on the effect of β1 on power ***

 - **Power decreased as β1 increased** between **-2 and 0** for any given sigma(σ) and sample size(n), means as β1 increases, reject hypothesis decreases
 - **Power increased as β1 increased** between **0 and 2**  for any given sigma(σ) and sample size(n), means as β1 increases, reject hypothesis increases

 
####[3.4.4] Are 1000 simulations sufficient

Let's simulate the MLR for 3000 iteration and record the Train / Test RMSE to see what's the change
```{r}
birthday = 19770411
set.seed(birthday)
no_loop = 3000

#Matrix to store the Power of the simulated models
model_sim_3000 = cbind(beta_1_sim_val=rep(0,length(beta_1)),n_10_sigma_1=rep(0,length(beta_1)),n_20_sigma_1=rep(0,length(beta_1)),n_30_sigma_1=rep(0,length(beta_1)),n_10_sigma_2=rep(0,length(beta_1)),n_20_sigma_2=rep(0,length(beta_1)),n_30_sigma_2=rep(0,length(beta_1)),n_10_sigma_4=rep(0,length(beta_1)),n_20_sigma_4=rep(0,length(beta_1)),n_30_sigma_1=rep(0,length(beta_1)))

model_sim_3000[,1] = seq(from=-2,to=2,by=0.1)
extra_counter = 0

#Loop to iterate over Sigma, number of samples, beta_1 values and 1000 simulations to extract and store the Power values
run_counter = 0
for(s in 1:length(sigma)){ #sigma = c(1,2,4)
  for (n_s in 1:length(n)){ #n = c(10,20,30)
    for (b in 1:length(beta_1)){ # beta_1 = seq(from=-2,to=2,by=0.1)
      tot_sum_signifincace_sim = 0
      run_counter = run_counter + 1      
      for(i in 1:no_loop){
        slr_data = slr_sim(n_in = n[n_s], sd = sigma[s], beta_1_in = beta_1[b] )
        model = lm(response~predictor, data = slr_data)
        model_p_val = summary(model)$coefficient[2,4]
        tot_sum_signifincace_sim = tot_sum_signifincace_sim + ifelse(model_p_val<alpha,1,0)
      }
      power_sim_beta = tot_sum_signifincace_sim / no_loop
      model_sim_3000[b,s+n_s+extra_counter] = power_sim_beta    
      print (paste(run_counter," Sigma:",sigma[s]," n:",n[n_s]," beta_1:",beta_1[b]," Signifiance_Sum:",tot_sum_signifincace_sim, " Power:",power_sim_beta))
    }
  }
  extra_counter = extra_counter + 2
}
```


```{r}
par(mfrow=c(1, 2))
plot(model_sim[,1],model_sim[,2], type="o", col="dodgerblue", pch=1, lty=1,main="1000 Iterations",xlab = "beta_1",ylab = "Power")
points(model_sim[,1], model_sim[,3], col="red", pch=2,lty=2)
lines(model_sim[,1], model_sim[,3], col="red",lty=2)
points(model_sim[,1], model_sim[,4], col="tan1", pch=3,lty=3)
lines(model_sim[,1], model_sim[,4], col="tan1",lty=3)

points(model_sim[,1],model_sim[,5], type="o", col="yellow", pch=4, lty=4)
lines(model_sim[,1],model_sim[,5], type="o", col="yellow", pch=4, lty=4)

points(model_sim[,1], model_sim[,6], col="deeppink", pch=5,lty=5)
lines(model_sim[,1], model_sim[,6], col="deeppink",lty=5)
points(model_sim[,1], model_sim[,7], col="darkolivegreen4", pch=6,lty=6)
lines(model_sim[,1], model_sim[,7], col="darkolivegreen4",lty=6)

points(model_sim[,1],model_sim[,8], type="o", col="darkgreen", pch=7, lty=7)
lines(model_sim[,1],model_sim[,8], type="o", col="darkgreen", pch=7, lty=7)

points(model_sim[,1], model_sim[,9], col="darkblue", pch=8,lty=8)
lines(model_sim[,1], model_sim[,9], col="darkblue",lty=8)
points(model_sim[,1], model_sim[,10], col="blueviolet", pch=9,lty=9)
lines(model_sim[,1], model_sim[,10], col="blueviolet",lty=9)

legend("topright",legend=c("σ=1,n=10","σ=1,n=20","σ=1,n=30","σ=2,n=10","σ=2,n=20","σ=2,n=30","σ=3,n=10","σ=3,n=20","σ=3,n=30"),col=c("dodgerblue","red","darkorchid","yellow","deeppink","darkolivegreen4","darkgreen","darkblue","blueviolet"),pch=c(1,2,3,4,5,6,7,8,9),lty=c(1,2,3,4,5,6,7,8,9))

#Below code is to plot the simulated beta_1 for 3000 simulations. This plot will helps us to understands, whether there is anything changes because of increase the simulation
#from 1000 iteration to 3000 iteration
plot(model_sim_3000[,1],model_sim_3000[,2], type="o", col="dodgerblue", pch=1, lty=1,main="3000 Iterations",xlab = "beta_1",ylab = "Power")
points(model_sim_3000[,1], model_sim_3000[,3], col="red", pch=2,lty=2)
lines(model_sim_3000[,1], model_sim_3000[,3], col="red",lty=2)
points(model_sim_3000[,1], model_sim_3000[,4], col="tan1", pch=3,lty=3)
lines(model_sim_3000[,1], model_sim_3000[,4], col="tan1",lty=3)

points(model_sim_3000[,1],model_sim_3000[,5], type="o", col="yellow", pch=4, lty=4)
lines(model_sim_3000[,1],model_sim_3000[,5], type="o", col="yellow", pch=4, lty=4)

points(model_sim_3000[,1], model_sim_3000[,6], col="deeppink", pch=5,lty=5)
lines(model_sim_3000[,1], model_sim_3000[,6], col="deeppink",lty=5)
points(model_sim_3000[,1], model_sim_3000[,7], col="darkolivegreen4", pch=6,lty=6)
lines(model_sim_3000[,1], model_sim_3000[,7], col="darkolivegreen4",lty=6)

points(model_sim_3000[,1],model_sim_3000[,8], type="o", col="darkgreen", pch=7, lty=7)
lines(model_sim_3000[,1],model_sim_3000[,8], type="o", col="darkgreen", pch=7, lty=7)

points(model_sim_3000[,1], model_sim_3000[,9], col="darkblue", pch=8,lty=8)
lines(model_sim_3000[,1], model_sim_3000[,9], col="darkblue",lty=8)
points(model_sim_3000[,1], model_sim_3000[,10], col="blueviolet", pch=9,lty=9)
lines(model_sim_3000[,1], model_sim_3000[,10], col="blueviolet",lty=9)

legend("topright",legend=c("σ=1,n=10","σ=1,n=20","σ=1,n=30","σ=2,n=10","σ=2,n=20","σ=2,n=30","σ=3,n=10","σ=3,n=20","σ=3,n=30"),col=c("dodgerblue","red","darkorchid","yellow","deeppink","darkolivegreen4","darkgreen","darkblue","blueviolet"),pch=c(1,2,3,4,5,6,7,8,9),lty=c(1,2,3,4,5,6,7,8,9))
```

From the above plots its clearly visible that the plots for the 3000 iterations have much smoother curve with respect to the Power value, as compared to the 1000 iterations (where the power value is ups-down). Let's take the range of the Power for the 1000 vs 3000 iterations for a given value of sigma (let's say , sigma = 1) and for a particular value of sample size ( n= 10).

- Power for 1000 Iteration (sigma = 1, n = 10) is between **`r range(model_sim[,2])[1]`** and **`r range(model_sim[,2])[2]`**
- Power for 3000 Iteration (sigma = 1, n = 10)is between **`r range(model_sim_3000[,2])[1]`** and **`r range(model_sim_3000[,2])[2]`**


From the range prospective the 3000 iterative have slightly increased value as compared to 1000 iteration. Hence if we increase the iteration, the Power becomes more and more accurate to the Truth, hence I conclude that **1000 iteration is sufficient** to know the impact of beta(β1), sample size(n) and sigma(σ) on the Power, however **1000 iteration is not sufficient** to know the truth of Power 

 
 